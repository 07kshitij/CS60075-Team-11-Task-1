{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LCP_Shared_Task_ML_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_o3s1ep5Iwj"
      },
      "source": [
        "##Overall Architecture used for the LCP shared Task\n",
        "\n",
        "* Contains the individual ML Models used\n",
        "\n",
        "Note\n",
        "* The results in the output of the cells are the best results obtained using the checkpoints saved at the time of training. \n",
        "* Slight deviations are possible if everything is run from scratch due to certain non-determinism in PyTorch computations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnSmpv2kroCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1db02e2-809a-4ae3-b8ba-9f7f9e82b5fd"
      },
      "source": [
        "!pip install spacy-syllables\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip3 install wordfreq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy-syllables\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/71/0f9ea26326aba87950968177577da72e634773e49366eec0f023a7d88a1b/spacy_syllables-3.0.1-py3-none-any.whl\n",
            "Collecting pyphen<0.11.0,>=0.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 16.8MB/s \n",
            "\u001b[?25hCollecting spacy<4.0.0,>=3.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/70/a0b8bd0cb54d8739ba4d6fb3458785c3b9b812b7fbe93b0f10beb1a53ada/spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 292kB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (0.8.2)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/5c/493a2f3bb0eac17b1d48129ecfd251f0520b6c89493e9fd0522f534a9e4a/catalogue-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.0.5)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/54/76982427ceb495dd19ff982c966708c624b85e03c45bf1912feaf60c7b2d/srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.0.5)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/78/d8/e25bc7f99877de34def57d36769f0cce4e895b374cdc766718efc724f9ac/spacy_legacy-3.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (20.9)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/08/20e707519bcded1a0caa6fd024b767ac79e4e5d0fb92266bb7dcf735e338/thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 32.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.8.1)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.7.4.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (1.0.5)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (54.2.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (4.41.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (0.4.1)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<4.0.0,>=3.0.3->spacy-syllables) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.3->spacy-syllables) (1.1.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (7.1.2)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=202ac3854f626823ec3c1b02fe983afb06633d0f78466b7f2d145f8f40aa37f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: pyphen, catalogue, typer, smart-open, pathy, srsly, spacy-legacy, pydantic, thinc, spacy, spacy-syllables\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: smart-open 4.2.0\n",
            "    Uninstalling smart-open-4.2.0:\n",
            "      Successfully uninstalled smart-open-4.2.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.1 pathy-0.4.0 pydantic-1.7.3 pyphen-0.10.0 smart-open-3.0.0 spacy-3.0.5 spacy-legacy-3.0.2 spacy-syllables-3.0.1 srsly-2.4.0 thinc-8.0.2 typer-0.3.2\n",
            "2021-04-10 08:07:05.922324: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.8.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting wordfreq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/61/f069a02fd061175272ada29d2c472deaea777dd52445602b32333e8c3529/wordfreq-2.4.2.tar.gz (36.4MB)\n",
            "\u001b[K     |████████████████████████████████| 36.4MB 131kB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (1.0.2)\n",
            "Collecting langcodes>=3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/4f/70885b0b2afb5b092b465c6377d3c67847184e66a5c955881ba0b2902839/langcodes-3.1.0.tar.gz (168kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 52.1MB/s \n",
            "\u001b[?25hCollecting regex>=2020.04.04\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/28/5f08d8841013ccf72cd95dfff2500fe7fb39467af12c5e7b802d8381d811/regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720kB)\n",
            "\u001b[K     |████████████████████████████████| 727kB 44.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wordfreq, langcodes\n",
            "  Building wheel for wordfreq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordfreq: filename=wordfreq-2.4.2-cp37-none-any.whl size=36384934 sha256=ab31bfe06b3d212dc2e55a09104e8aaa6ece7d259fcbefa7a2f087f4074bda94\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/7f/2f/ce0b9835ee84fc6b392b1e4563c77270c39a6882e74d22373b\n",
            "  Building wheel for langcodes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langcodes: filename=langcodes-3.1.0-cp37-none-any.whl size=165888 sha256=6b5bc582663358ce6fb80e736bd458704acbd4a1d1c161ab0a0f11fe08bfa247\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/57/10/167a0f1c07409a7b72d8d08391fa9739ef37e834695575ccc8\n",
            "Successfully built wordfreq langcodes\n",
            "Installing collected packages: langcodes, regex, wordfreq\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed langcodes-3.1.0 regex-2021.4.4 wordfreq-2.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv10GoDvxaeM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from wordfreq import word_frequency\n",
        "from scipy import stats\n",
        "import csv\n",
        "import spacy\n",
        "from spacy_syllables import SpacySyllables\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5t3kQZm4L1T"
      },
      "source": [
        "# We saved certain models while training (biLSTM to predict context probability) to avoid re-training again\n",
        "\n",
        "# Steps To use already trained models\n",
        "# 1. Uncomment the below cell two lines\n",
        "# 2. Download the TrainedModels folder in the repo\n",
        "# 3. Upload the whole folder to your google drive\n",
        "# 4. Run the below cell and authenticate the Google Drive request\n",
        "# 5. The cells later where trained and saved models can be used have instructions on how to use them"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d73e77vIu_k7",
        "outputId": "b13315bb-2c82-4035-c0ba-56c3215bb3b0"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDhaYmV9ULB7",
        "outputId": "55a5199e-608a-4f6d-bca4-c12aea5afc85"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-10 08:07:28--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-04-10 08:07:28--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-04-10 08:07:28--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.30MB/s    in 2m 41s  \n",
            "\n",
            "2021-04-10 08:10:09 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeSBDTRwUl6o",
        "outputId": "1df1ca05-d48a-40ef-ceed-09d11a1eccdb"
      },
      "source": [
        "!unzip glove*.zip\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrgVK4JYrsm5"
      },
      "source": [
        "# https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch\n",
        "# Seed all rngs for deterministic results\n",
        "def seed_all(seed = 0):\n",
        "  random.seed(0)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw8rWJgy0Y7Y"
      },
      "source": [
        "seed_all(0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5RpElbs_7jn"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I-7XsrzA_uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d01aac36-cc65-4d4c-9074-e19dfe471141"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"syllables\", after='tagger') # Add the syllable tagger pipe"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy_syllables.SpacySyllables at 0x7f099b952590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPyP8nfVAxIw"
      },
      "source": [
        "SINGLE_TRAIN_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/train/lcp_single_train.tsv\"\n",
        "SINGLE_TEST_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/test-labels/lcp_single_test.tsv\"\n",
        "\n",
        "MULTI_TRAIN_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/train/lcp_multi_train.tsv\"\n",
        "MULTI_TEST_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/test-labels/lcp_multi_test.tsv\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZQb1lbDA9o2"
      },
      "source": [
        "def get_data_frames():\n",
        "  df_train_single = pd.read_csv(SINGLE_TRAIN_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "  df_test_single = pd.read_csv(SINGLE_TEST_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "\n",
        "  df_train_multi = pd.read_csv(MULTI_TRAIN_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "  df_test_multi = pd.read_csv(MULTI_TEST_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "\n",
        "  return df_train_single, df_test_single, df_train_multi, df_test_multi"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLkKx7GsyGcK"
      },
      "source": [
        "df_train_single, df_test_single, df_train_multi, df_test_multi = get_data_frames()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYq6zO2NPEBt"
      },
      "source": [
        "Features used \n",
        "\n",
        "* Word Embedding [GloVe 50 dimensional embeddings](http://nlp.stanford.edu/data/glove.6B.zip)\n",
        "* Length of word\n",
        "* Syllable count [PyPy](https://pypi.org/project/syllables/)\n",
        "* Word Frequency [PyPy](https://pypi.org/project/wordfreq/)\n",
        "* POS tag [Spacy](https://spacy.io/usage/linguistic-features#pos-tagging)\n",
        "\n",
        "[Reference](https://www.aclweb.org/anthology/W18-0508.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DOS3snOBoin"
      },
      "source": [
        "single_tokens_train_raw = df_train_single[\"token\"].astype(str).to_list()\n",
        "single_tokens_test_raw = df_test_single[\"token\"].astype(str).to_list()\n",
        "\n",
        "y_single_train = df_train_single[\"complexity\"].astype(np.float32).to_numpy()\n",
        "y_single_test = df_test_single[\"complexity\"].astype(np.float32).to_numpy()\n",
        "\n",
        "multi_tokens_train_raw = df_train_multi[\"token\"].astype(str).to_list()\n",
        "multi_tokens_test_raw = df_test_multi[\"token\"].astype(str).to_list()\n",
        "\n",
        "y_multi_train = df_train_multi[\"complexity\"].astype(np.float32).to_numpy()\n",
        "y_multi_test = df_test_multi[\"complexity\"].astype(np.float32).to_numpy()\n",
        "\n",
        "sent_train_single_raw = df_train_single[\"sentence\"].to_list()\n",
        "sent_test_single_raw = df_test_single[\"sentence\"].to_list()\n",
        "\n",
        "sent_train_multi_raw = df_train_multi[\"sentence\"].to_list()\n",
        "sent_test_multi_raw = df_test_multi[\"sentence\"].to_list()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z00nnMqBVTmi"
      },
      "source": [
        "EMBEDDING_DIM = 50\n",
        "\n",
        "def get_embeddings():\n",
        "  embedding_index = {}\n",
        "  with open('glove.6B.{}d.txt'.format(EMBEDDING_DIM), 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      token = values[0]\n",
        "      embedding_index[token] = np.asarray(values[1:], dtype='float32')\n",
        "  return embedding_index"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcpQP9sYzfun",
        "outputId": "7177dd52-0534-462e-c363-c1167de60485"
      },
      "source": [
        "embedding_index = get_embeddings()\n",
        "print('Token count in embeddings: {}'.format(len(embedding_index)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token count in embeddings: 400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXTRtrC3ptjk"
      },
      "source": [
        "biLSTM to predict target probability\n",
        "\n",
        "Reference - [PyTorch](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eykntycSwUQO"
      },
      "source": [
        "HIDDEN_DIM = 10"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOcp3gYaRFyv"
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "  seq = seq.split()\n",
        "  idxs = [to_ix[w.lower()] if w.lower() in to_ix else len(to_ix) for w in seq]\n",
        "  idxs = torch.tensor(idxs)\n",
        "  idxs = nn.functional.one_hot(idxs, num_classes=len(to_ix))\n",
        "  idxs = torch.tensor(idxs, dtype=torch.float32)\n",
        "  return idxs\n",
        "\n",
        "\n",
        "def map_token_to_idx():\n",
        "  word_to_ix = {}\n",
        "  word_to_ix_multi = {}\n",
        "  for sent in sent_train_single_raw:\n",
        "    sent = sent.split()\n",
        "    for word in sent:\n",
        "      word = word.lower()\n",
        "      if word not in word_to_ix:\n",
        "        word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "  for sent in sent_train_multi_raw:\n",
        "    sent = sent.split()\n",
        "    for word in sent:\n",
        "      word = word.lower()\n",
        "      if word not in word_to_ix_multi:\n",
        "        word_to_ix_multi[word] = len(word_to_ix_multi)\n",
        "  \n",
        "  return word_to_ix, word_to_ix_multi"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Td2vdR0JqZ",
        "outputId": "8afc270d-1f82-4e47-f80c-97267e18d510"
      },
      "source": [
        "word_to_ix, word_to_ix_multi = map_token_to_idx()\n",
        "print('SWE vocab size: {}\\nMWE vocab size: {}'.format(len(word_to_ix), len(word_to_ix_multi)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SWE vocab size: 24350\n",
            "MWE vocab size: 9699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOFJH3_us5bj"
      },
      "source": [
        "biLSTM class to calculate token probability given context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8rD4jq599bT"
      },
      "source": [
        "class biLSTM(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size):\n",
        "    super(biLSTM, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "    self.hidden2tag = nn.Linear(2 * hidden_dim, output_size)\n",
        "\n",
        "  def prepare_embedding(self, sentence):\n",
        "    embeddings = []\n",
        "    for word in sentence:\n",
        "      word = word.lower()\n",
        "      if word in embedding_index:\n",
        "        embeddings.extend(embedding_index[word])\n",
        "      else:\n",
        "        embeddings.extend(np.random.random(EMBEDDING_DIM).tolist())\n",
        "    embeddings = torch.tensor(embeddings, dtype=torch.float32, device=device)\n",
        "    return embeddings\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    sentence = sentence.split()\n",
        "    embeds = self.prepare_embedding(sentence)\n",
        "    lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "    tag_scores = F.softmax(tag_space, dim=1)\n",
        "    return tag_scores"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrolGYkytD4f"
      },
      "source": [
        "biLSTM model for single word targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8of3r04B5Tn"
      },
      "source": [
        "model = biLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(word_to_ix))\n",
        "# Used while training phase to avoid re training the same model again and again\n",
        "# To use the saved model, change the False in next cell to True (Ensure you've uploaded the model in your Google Drive using steps mentioned earlier)\n",
        "path_biLSTM_single = '/content/gdrive/My Drive/TrainedModels/biLSTM.pt'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCAl7MWG4gqN"
      },
      "source": [
        "USE_PRETRAINED_SINGLE_WORD_TARGET_MODEL = False"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DfKvyAUT8VN",
        "outputId": "c4f5993a-1aa7-4b86-9785-e45507b69a70"
      },
      "source": [
        "if USE_PRETRAINED_SINGLE_WORD_TARGET_MODEL:\n",
        "  print('Using pre-trained biLSTM on single target expressions')\n",
        "  model = torch.load(path_biLSTM_single)\n",
        "  model.eval()\n",
        "else:\n",
        "  print('Training biLSTM on single target expressions')\n",
        "  # Train the model for 10 epochs\n",
        "  model = biLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(word_to_ix))\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "  for epoch in range(10):\n",
        "    loss_sum = 0\n",
        "    for sentence in sent_train_single_raw:\n",
        "      model.zero_grad()\n",
        "      targets = prepare_sequence(sentence, word_to_ix)\n",
        "      tag_scores = model(sentence)\n",
        "      loss = loss_function(tag_scores, targets)\n",
        "      loss_sum += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print('Epoch: {} Loss: {}'.format(epoch, loss_sum.item()))\n",
        "  # path_biLSTM_single = '/content/gdrive/My Drive/TrainedModels/biLSTM.pt'\n",
        "  # torch.save(model, path_biLSTM_single)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pre-trained biLSTM on single target expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYMs37iTtLDn"
      },
      "source": [
        "biLSTM model for multi word targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1Ypxmd65_GL"
      },
      "source": [
        "model_multi = biLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix_multi), len(word_to_ix_multi))\n",
        "# Used while training phase to avoid re training the same model again and again\n",
        "# To use the saved model, change the False in next cell to True (Ensure you've uploaded the model in your Google Drive using steps mentioned earlier)\n",
        "path_biLSTM_multi = '/content/gdrive/My Drive/TrainedModels/biLSTM_multi.pt'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSw5V4vi6Hx0"
      },
      "source": [
        "USE_PRETRAINED_MULTI_WORD_TARGET_MODEL = False"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2QK4FDT6eL0",
        "outputId": "afceb437-e7ef-4b19-8d8b-7b98a07438ba"
      },
      "source": [
        "if USE_PRETRAINED_MULTI_WORD_TARGET_MODEL:\n",
        "  print('Using pre-trained biLSTM on multi target expressions')\n",
        "  model_multi = torch.load(path_biLSTM_multi)\n",
        "  model_multi.eval()\n",
        "else:\n",
        "  print('Training biLSTM on multi target expressions')\n",
        "  model_multi = biLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix_multi), len(word_to_ix_multi))\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model_multi.parameters(), lr=0.01)\n",
        "  for epoch in range(10):\n",
        "    loss_sum = 0\n",
        "    for sentence in sent_train_multi_raw:\n",
        "      model_multi.zero_grad()\n",
        "      targets = prepare_sequence(sentence, word_to_ix_multi)\n",
        "      tag_scores = model_multi(sentence)\n",
        "      loss = loss_function(tag_scores, targets)\n",
        "      loss_sum += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print('Epoch: {} Loss: {}'.format(epoch, loss_sum.item()))\n",
        "  # path_biLSTM_multi = '/content/gdrive/My Drive/TrainedModels/biLSTM_multi.pt'\n",
        "  # torch.save(model_multi, path_biLSTM_multi)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pre-trained biLSTM on multi target expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5PUts_pSloU"
      },
      "source": [
        "def prepare_features_single_word(tokens, sentences):\n",
        "  features = []\n",
        "  for idx, word in enumerate(tokens):\n",
        "    word = word.lower()\n",
        "    feature = []\n",
        "\n",
        "    # Word length\n",
        "    feature.append(len(word))\n",
        "    doc = nlp(word)\n",
        "\n",
        "    # Syllable count and word frequency in the corpus\n",
        "    # Spacy tokenizes the input sentence\n",
        "    # In this case we would have only one token, the target word\n",
        "    for token in doc:\n",
        "      feature.append(token._.syllables_count)\n",
        "      feature.append(word_frequency(word, 'en'))\n",
        "\n",
        "    # Probability of target word `word` in the sentence estimated from by `model`\n",
        "    if word in word_to_ix:\n",
        "      # Output scores for each of the word in the sentence\n",
        "      out = model(sentences[idx])\n",
        "      pos = -1\n",
        "      for itr, token in enumerate(sentences[idx].split()):\n",
        "        if token.lower() == word:\n",
        "          pos = itr\n",
        "          break\n",
        "      id_pos = word_to_ix[word] # word to id mapping\n",
        "      feature.append(float(out[pos][id_pos]))\n",
        "    else:\n",
        "      # `word` not in vocabulary, so cannot predict probability in context\n",
        "      feature.append(0.0)\n",
        "\n",
        "    # GloVE embedding for the `word`\n",
        "    if word in embedding_index:\n",
        "      feature.extend(embedding_index[word].tolist())\n",
        "    else:\n",
        "      # `word` not in the GloVE corpus, take a random embedding\n",
        "      feature.extend(np.random.random(EMBEDDING_DIM).tolist())\n",
        "    features.append(feature)\n",
        "\n",
        "    if (idx + 1) % 500 == 0:\n",
        "      print('Prepared features for {} single target word sentences'.format(idx + 1))\n",
        "  return features"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BY3iu53udY"
      },
      "source": [
        "def prepare_features_multi_word(tokens, sentences):\n",
        "  features = []\n",
        "  for idx, word in enumerate(tokens):\n",
        "    word = word.lower()\n",
        "    feature = []\n",
        "    doc = nlp(word)\n",
        "    word = word.split(' ')\n",
        "    assert(len(word) == 2)\n",
        "\n",
        "    # MWE length = sum(length of individual words)\n",
        "    feature.append(len(word[0]) + len(word[1]))\n",
        "\n",
        "    syllables = 0\n",
        "    probability = 1\n",
        "    embedding = np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "    # Syllable count and word frequency in the corpus\n",
        "    # Spacy tokenizes the input sentence\n",
        "    # In this case we would have two tokens\n",
        "\n",
        "    for token in doc:\n",
        "      word_ = token.text\n",
        "      syllables += token._.syllables_count\n",
        "      probability *= word_frequency(word_, 'en')\n",
        "\n",
        "      # GloVE embedding current `word_` of the MWE\n",
        "      if word_ in embedding_index:\n",
        "        embedding = embedding + embedding_index[word_]\n",
        "      else:\n",
        "        # `word_` not in the GloVE corpus, take a random embedding\n",
        "        embedding = embedding + np.random.random(EMBEDDING_DIM)\n",
        "\n",
        "    # Average embedding of the two tokens in the MWE\n",
        "    embedding = embedding / 2\n",
        "    feature.append(syllables)\n",
        "    feature.append(probability)\n",
        "\n",
        "    # Product of probabilities of constituent words in the MWE\n",
        "    if word[0] in word_to_ix_multi and word[1] in word_to_ix_multi:\n",
        "      # Output scores for each of the word in the sentence\n",
        "      out = model_multi(sentences[idx])\n",
        "      pos0, pos1 = -1, -1\n",
        "      for itr, token in enumerate(sentences[idx].split()):\n",
        "        if token.lower() == word[0]:\n",
        "          pos0 = itr\n",
        "          pos1 = itr + 1\n",
        "          break\n",
        "      id_pos0 = word_to_ix_multi[word[0]]\n",
        "      id_pos1 = word_to_ix_multi[word[1]]\n",
        "      feature.append(float(out[pos0][id_pos0] * out[pos1][id_pos1]))\n",
        "    else:\n",
        "      # Either of the constituent words of the MWE not in vocabulary \\\n",
        "      # So cannot predict probability in context\n",
        "      feature.append(0.0)\n",
        "\n",
        "    feature.extend(embedding.tolist())\n",
        "    features.append(feature)\n",
        "\n",
        "    if (idx + 1) % 500 == 0:\n",
        "      print('Prepared features for {} multi target word sentences'.format(idx + 1))\n",
        "\n",
        "  return features"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYuU10C8hZjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a51cb0-2247-41cc-cfbb-7cd27e9fa1a7"
      },
      "source": [
        "print('+++ Generating Train features for Single word expressions +++')\n",
        "features_train_single = prepare_features_single_word(single_tokens_train_raw, sent_train_single_raw)\n",
        "print('+++ Generating Test features for Single word expressions +++')\n",
        "features_test_single = prepare_features_single_word(single_tokens_test_raw, sent_test_single_raw)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ Generating Train features for Single word expressions +++\n",
            "Prepared features for 500 single target word sentences\n",
            "Prepared features for 1000 single target word sentences\n",
            "Prepared features for 1500 single target word sentences\n",
            "Prepared features for 2000 single target word sentences\n",
            "Prepared features for 2500 single target word sentences\n",
            "Prepared features for 3000 single target word sentences\n",
            "Prepared features for 3500 single target word sentences\n",
            "Prepared features for 4000 single target word sentences\n",
            "Prepared features for 4500 single target word sentences\n",
            "Prepared features for 5000 single target word sentences\n",
            "Prepared features for 5500 single target word sentences\n",
            "Prepared features for 6000 single target word sentences\n",
            "Prepared features for 6500 single target word sentences\n",
            "Prepared features for 7000 single target word sentences\n",
            "Prepared features for 7500 single target word sentences\n",
            "+++ Generating Test features for Single word expressions +++\n",
            "Prepared features for 500 single target word sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqj-99dkA43o",
        "outputId": "99abe38c-94a0-4cf1-86f7-e540d8088d4e"
      },
      "source": [
        "print('+++ Generating Train features for Multi word expressions +++')\n",
        "features_train_multi = prepare_features_multi_word(multi_tokens_train_raw, sent_train_multi_raw)\n",
        "print('+++ Generating Test features for Multi word expressions +++')\n",
        "features_test_multi = prepare_features_multi_word(multi_tokens_test_raw, sent_test_multi_raw)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ Generating Train features for Multi word expressions +++\n",
            "Prepared features for 500 multi target word sentences\n",
            "Prepared features for 1000 multi target word sentences\n",
            "Prepared features for 1500 multi target word sentences\n",
            "+++ Generating Test features for Multi word expressions +++\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESn2VJdU1p8h"
      },
      "source": [
        "# Convert all features to torch.tensor to enable use in PyTorch models\n",
        "X_train_single_tensor = torch.tensor(features_train_single, dtype=torch.float32, device=device)\n",
        "X_test_single_tensor = torch.tensor(features_test_single, dtype=torch.float32, device=device)\n",
        "X_train_multi_tensor = torch.tensor(features_train_multi, dtype=torch.float32, device=device)\n",
        "X_test_multi_tensor = torch.tensor(features_test_multi, dtype=torch.float32, device=device)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VikNi2ZmtEn"
      },
      "source": [
        "# Reshape all output complexity scores to single dimension vectors\n",
        "y_single_train = y_single_train.reshape(y_single_train.shape[0], -1)\n",
        "y_single_test = y_single_test.reshape(y_single_test.shape[0], -1)\n",
        "y_multi_train = y_multi_train.reshape(y_multi_train.shape[0], -1)\n",
        "y_multi_test = y_multi_test.reshape(y_multi_test.shape[0], -1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhjYgCiA13Z1"
      },
      "source": [
        "# Convert all target outputs to torch.tensor to enable use in PyTorch models\n",
        "Y_train_single_tensor = torch.tensor(y_single_train, dtype=torch.float32, device=device)\n",
        "Y_test_single_tensor = torch.tensor(y_single_test, dtype=torch.float32, device=device)\n",
        "Y_train_multi_tensor = torch.tensor(y_multi_train, dtype=torch.float32, device=device)\n",
        "Y_test_multi_tensor = torch.tensor(y_multi_test, dtype=torch.float32, device=device)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSRoIvSU2McF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99337858-3d9b-409e-b92e-0efcd8502035"
      },
      "source": [
        "# Ensure each sample from test and train for single word expression is taken\n",
        "print(X_train_single_tensor.shape)\n",
        "print(X_test_single_tensor.shape)\n",
        "print(Y_train_single_tensor.shape)\n",
        "print(Y_test_single_tensor.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([7662, 54])\n",
            "torch.Size([917, 54])\n",
            "torch.Size([7662, 1])\n",
            "torch.Size([917, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7de9O0tZBvqD",
        "outputId": "aa2d3d90-fc71-4244-84e2-719f83363415"
      },
      "source": [
        "# Ensure each sample from test and train for multi word expression is taken\n",
        "print(X_train_multi_tensor.shape)\n",
        "print(X_test_multi_tensor.shape)\n",
        "print(Y_train_multi_tensor.shape)\n",
        "print(Y_test_multi_tensor.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1517, 54])\n",
            "torch.Size([184, 54])\n",
            "torch.Size([1517, 1])\n",
            "torch.Size([184, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONmt2TBCK9jO"
      },
      "source": [
        "def convert_tensor_to_np(y):\n",
        "  if device == torch.device(\"cuda\"):\n",
        "    y = y.cpu()\n",
        "  y = y.detach().numpy()\n",
        "  return y"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m07BB6hLc70"
      },
      "source": [
        "from copy import deepcopy"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QVP68p13j5C"
      },
      "source": [
        "# Evaluate the metrics upon which the model would be evaluated\n",
        "def evaluate_metrics(labels, predicted):\n",
        "  vx, vy = [], []\n",
        "  if torch.is_tensor(labels):\n",
        "    vx = labels.clone()\n",
        "    vx = convert_tensor_to_np(vx)\n",
        "  else:\n",
        "    vx = deepcopy(labels)\n",
        "  if torch.is_tensor(predicted):\n",
        "    vy = predicted.clone()\n",
        "    vy = convert_tensor_to_np(vy)\n",
        "  else:\n",
        "    vy = deepcopy(predicted)\n",
        "\n",
        "  pearsonR = np.corrcoef(vx.T, vy.T)[0, 1]\n",
        "  spearmanRho = stats.spearmanr(vx, vy)\n",
        "  MSE = np.mean((vx - vy) ** 2)\n",
        "  MAE = np.mean(np.absolute(vx - vy))\n",
        "  RSquared = pearsonR ** 2\n",
        "\n",
        "  print(\"Peason's R: {}\".format(pearsonR))\n",
        "  print(\"Spearman's rho: {}\".format(spearmanRho))\n",
        "  print(\"R Squared: {}\".format(RSquared))\n",
        "  print(\"MSE: {}\".format(MSE))\n",
        "  print(\"MAE: {}\".format(MAE))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGBqpNE_tUWK"
      },
      "source": [
        "## Neural Network\n",
        "\n",
        "* $N$ input sentences  \n",
        "\n",
        "* d (=EMBEDDING_DIM) word embedding\n",
        "\n",
        "* $I$ = Word Embedding matrix ($N \\times d$)\n",
        "\n",
        "* $W_1, W_2, W_3, W_4 := (d \\times 256), (256 \\times 128), (128 \\times 64), (64 \\times 1)$\n",
        "\n",
        "* Equations\n",
        "\n",
        "  * $o_1 = tanh(I \\times W_1 + b_1)$\n",
        "\n",
        "  * $o_2 = tanh(o_1 \\times W_2 + b_2)$\n",
        "\n",
        "  * $o_3 = tanh(o_2 \\times W_3 + b_3)$\n",
        "\n",
        "  * $o_4 = \\sigma(o_3 \\times W_4)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLhlNCoX2UrA"
      },
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(embedding_dim, 128, bias=True)\n",
        "    self.linear2 = nn.Linear(128, 256, bias=True)\n",
        "    self.linear3 = nn.Linear(256, 64, bias=True)\n",
        "    self.linear4 = nn.Linear(64, 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = torch.tanh(self.linear1(input))\n",
        "    out = torch.tanh(self.linear2(out))\n",
        "    out = torch.tanh(self.linear3(out))\n",
        "    out = torch.sigmoid(self.linear4(out))\n",
        "    return out"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwSTHAPd28r8"
      },
      "source": [
        "loss_function = nn.MSELoss()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAW0QJDkCWvv"
      },
      "source": [
        "embedding_dim = X_train_single_tensor.shape[1]\n",
        "model_NN = NN(embedding_dim)\n",
        "model_NN.to(device)\n",
        "# Used while training phase to save the best checkpoint (with the best Pearson R on test set)\n",
        "# To use the saved model, change the False in next cell to True (Ensure you've uploaded the model in your Google Drive using steps mentioned earlier)\n",
        "path_NN = '/content/gdrive/My Drive/TrainedModels/NN_0.731.pt'"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lUaRjKGCkoJ"
      },
      "source": [
        "USE_PRETRAINED_SINGLE_WORD_TARGET_NN = False"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVCTc8Zf3KoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "207064d8-cef0-4a25-ea20-dbab16eaa543"
      },
      "source": [
        "if USE_PRETRAINED_SINGLE_WORD_TARGET_NN:\n",
        "  print('Using pre-trained NN on single target expressions')\n",
        "  model_NN = torch.load(path_NN)\n",
        "  model_NN.eval()\n",
        "else:\n",
        "  print('Training NN on single target expressions...')\n",
        "  model_NN = NN(embedding_dim)\n",
        "  model_NN.to(device)\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model_NN.parameters(), lr=0.002)\n",
        "  for epoch in range(30):\n",
        "    optimizer.zero_grad()\n",
        "    out = model_NN(X_train_single_tensor)\n",
        "    loss = loss_function(out, Y_train_single_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Epoch {} : {}\".format(epoch + 1, loss.item()))\n",
        "  # path_NN = '/content/gdrive/My Drive/TrainedModels/NN_0.731.pt'\n",
        "  # torch.save(model_NN, path_NN)  "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pre-trained NN on single target expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DMsEM5R3cUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf137819-f54e-4f34-b86c-99cbaea98390"
      },
      "source": [
        "out_NN = model_NN(X_test_single_tensor)\n",
        "evaluate_metrics(out_NN, Y_test_single_tensor)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peason's R: 0.73164611615449\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7083250660997157, pvalue=1.454658192615246e-140)\n",
            "R Squared: 0.5353060392839495\n",
            "MSE: 0.00759829580783844\n",
            "MAE: 0.06738824397325516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np7uf1kr-SEi",
        "outputId": "0c561eaa-9cb2-4624-bede-cb51fc2e7145"
      },
      "source": [
        "out_NN[0]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1671], grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YOGT0TCELKo"
      },
      "source": [
        "embedding_dim = X_train_multi_tensor.shape[1]\n",
        "model_NN_multi = NN(embedding_dim)\n",
        "model_NN_multi.to(device)\n",
        "# Used while training phase to save the best checkpoint (with the best Pearson R on test set)\n",
        "# To use the saved model, change the False in next cell to True (Ensure you've uploaded the model in your Google Drive using steps mentioned earlier)\n",
        "path_NN_multi = '/content/gdrive/My Drive/TrainedModels/NN_multi_0.775.pt'"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNZIIOkfDM7E"
      },
      "source": [
        "USE_PRETRAINED_MULTI_WORD_TARGET_NN = False"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl6L4RlYB-Wj",
        "outputId": "1d2d7558-0676-4a55-a239-b4427d0e9e35"
      },
      "source": [
        "if USE_PRETRAINED_MULTI_WORD_TARGET_NN:\n",
        "  print('Using pre-trained NN on multi target expressions')\n",
        "  model_NN_multi = torch.load(path_NN_multi)\n",
        "  model_NN_multi.eval()\n",
        "else:\n",
        "  print('Training NN on multi target expressions...')\n",
        "  model_NN_multi = NN(embedding_dim)\n",
        "  model_NN_multi.to(device)\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model_NN_multi.parameters(), lr=0.002)\n",
        "  for epoch in range(30):\n",
        "    optimizer.zero_grad()\n",
        "    out = model_NN_multi(X_train_multi_tensor)\n",
        "    loss = loss_function(out, Y_train_multi_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Epoch {} : {}\".format(epoch + 1, loss.item()))\n",
        "  # path_NN_multi = '/content/gdrive/My Drive/TrainedModels/NN_multi_0.774.pt'\n",
        "  # torch.save(model_NN_multi, path_NN_multi)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pre-trained NN on multi target expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msBIen6KCGY2",
        "outputId": "5c50b76c-c2e3-4e86-84e1-783a5384f7b2"
      },
      "source": [
        "out_NN_multi = model_NN_multi(X_test_multi_tensor)\n",
        "evaluate_metrics(out_NN_multi, Y_test_multi_tensor)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peason's R: 0.775311152864301\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7682863746456144, pvalue=4.1883306755893856e-37)\n",
            "R Squared: 0.6011073837557714\n",
            "MSE: 0.010189074091613293\n",
            "MAE: 0.07908465713262558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SpGwzvfAQ0y"
      },
      "source": [
        "## Machine Learning Methods\n",
        "\n",
        "* Linear Regression\n",
        "\n",
        "* Support Vector Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8RRwfzp0R-K"
      },
      "source": [
        "X_train_single_np = np.array(features_train_single)\n",
        "X_test_single_np = np.array(features_test_single)\n",
        "Y_train_single_np = np.array(y_single_train.reshape(y_single_train.shape[0], -1))\n",
        "Y_test_single_np = np.array(y_single_test.reshape(y_single_test.shape[0], -1))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2LzJmZwoFMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d1c6d8e-ee3f-40e5-f7c6-e92b95411002"
      },
      "source": [
        "print(X_train_single_np.shape)\n",
        "print(X_test_single_np.shape)\n",
        "print(Y_train_single_np.shape)\n",
        "print(Y_test_single_np.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7662, 54)\n",
            "(917, 54)\n",
            "(7662, 1)\n",
            "(917, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AW_YHxS73xW"
      },
      "source": [
        "X_train_multi_np = np.array(features_train_multi)\n",
        "X_test_multi_np = np.array(features_test_multi)\n",
        "Y_train_multi_np = np.array(y_multi_train.reshape(y_multi_train.shape[0], -1))\n",
        "Y_test_multi_np = np.array(y_multi_test.reshape(y_multi_test.shape[0], -1))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQP30U5JZ6Qi",
        "outputId": "0151ba11-6ac9-46cb-f142-c547865ba3ac"
      },
      "source": [
        "print(X_train_multi_np.shape)\n",
        "print(X_test_multi_np.shape)\n",
        "print(Y_train_multi_np.shape)\n",
        "print(Y_test_multi_np.shape)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1517, 54)\n",
            "(184, 54)\n",
            "(1517, 1)\n",
            "(184, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwsHCYnr3D3E"
      },
      "source": [
        "### Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDTeV2p3105R"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ATmatOhioJI"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPRzjdJtOWlM"
      },
      "source": [
        "def evaluateLinearRegression(X_train, Y_train, X_test, Y_test):\n",
        "  reg = make_pipeline(StandardScaler(), LinearRegression())\n",
        "  reg.fit(X_train, Y_train)\n",
        "  out = reg.predict(X_test)\n",
        "  out = out.reshape((out.shape[0], 1))\n",
        "  evaluate_metrics(out, Y_test)\n",
        "  return out"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMSTucrSOpk7",
        "outputId": "df7287d4-b26f-4b5c-c068-e7897be5c020"
      },
      "source": [
        "print('Linear Regression for Single word expressions')\n",
        "out_LR = evaluateLinearRegression(X_train_single_np, Y_train_single_np, X_test_single_np, Y_test_single_np)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression for Single word expressions\n",
            "Peason's R: 0.7108434116187733\n",
            "Spearman's rho: SpearmanrResult(correlation=0.6918562248147357, pvalue=1.4505768829498638e-131)\n",
            "R Squared: 0.5052983558418167\n",
            "MSE: 0.00801003205435963\n",
            "MAE: 0.06855075784165898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoDpEh5ZPA6B",
        "outputId": "bf292be2-dc11-4761-f59d-a2d7213b5f72"
      },
      "source": [
        "print('Linear Regression for Multi word expressions')\n",
        "out_LR_multi = evaluateLinearRegression(X_train_multi_np, Y_train_multi_np, X_test_multi_np, Y_test_multi_np)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression for Multi word expressions\n",
            "Peason's R: 0.7672485716855574\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7573872177422218, pvalue=1.5861218509138286e-35)\n",
            "R Squared: 0.5886703707535279\n",
            "MSE: 0.00994057080022215\n",
            "MAE: 0.08001505070711726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot7P6ba-3Gwx"
      },
      "source": [
        "### Support Vector Regressor\n",
        "\n",
        "* Radial basis function\n",
        "* C = 0.05\n",
        "* epsilon = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I21y5EAG2BHY"
      },
      "source": [
        "from sklearn.svm import SVR"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyTiLWD33PH1"
      },
      "source": [
        "def evaluateSVR(X_train, Y_train, X_test, Y_test):\n",
        "  svr = make_pipeline(StandardScaler(), SVR(C=0.05, epsilon=0.01))\n",
        "  svr.fit(X_train, Y_train.reshape(-1))\n",
        "  out = svr.predict(X_test)\n",
        "  out = out.reshape((out.shape[0], 1))\n",
        "  evaluate_metrics(out, Y_test)\n",
        "  return out"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffGoiNKq3hBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053a8666-9a9c-4a66-e511-50f412bb04b0"
      },
      "source": [
        "print('SVR for Single word expressions')\n",
        "out_svr = evaluateSVR(X_train_single_np, Y_train_single_np, X_test_single_np, Y_test_single_np)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVR for Single word expressions\n",
            "Peason's R: 0.732967844251433\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7037429683169284, pvalue=5.357912593044118e-138)\n",
            "R Squared: 0.537241860706593\n",
            "MSE: 0.007531175064386141\n",
            "MAE: 0.06719389225485689\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWFRhtSEPl9t",
        "outputId": "fd2df399-e131-4198-dd9b-5f3dc747b4ee"
      },
      "source": [
        "print('SVR for Multi word expressions')\n",
        "out_svr_multi = evaluateSVR(X_train_multi_np, Y_train_multi_np, X_test_multi_np, Y_test_multi_np)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVR for Multi word expressions\n",
            "Peason's R: 0.7834043625882116\n",
            "Spearman's rho: SpearmanrResult(correlation=0.754343227255628, pvalue=4.2297035272986037e-35)\n",
            "R Squared: 0.613722395322242\n",
            "MSE: 0.010053853343430836\n",
            "MAE: 0.0810940654531415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhJgiYbCCpJf"
      },
      "source": [
        "### Summary \n",
        "\n",
        "\n",
        "| Model | Type | Pearson | MSE | MAE|\n",
        "|---|---|---|---|---|\n",
        "|Neural Network| Single word| 0.7315 | 0.0077 | 0.0681|\n",
        "|Neural Network| Multi word| 0.7753 | 0.0100 | 0.0780|\n",
        "|Linear Regression| Single word| 0.7108 | 0.0080 | 0.0685|\n",
        "|Linear Regression| Multi word| 0.7672 | 0.0099 | 0.0800|\n",
        "|SVR| Single word| 0.7330 | 0.0075 | 0.0672|\n",
        "|SVR| Multi word| 0.7834 | 0.0100 | 0.0811|\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZzXRaFGpKVj"
      },
      "source": [
        "single_ids = df_test_single[\"id\"].astype(str).to_list()\n",
        "multi_ids = df_test_multi[\"id\"].astype(str).to_list()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfTrx7qnSiYU"
      },
      "source": [
        "Aggregation of results obtained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Va00qQwAtaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9055815-3ef2-4211-95e2-15f75149afcf"
      },
      "source": [
        "out_ensemble = []\n",
        "\n",
        "for idx in range(len(out_NN)):\n",
        "  score = 0\n",
        "  score += float(out_NN[idx])\n",
        "  score += float(out_LR[idx])\n",
        "  score += float(out_svr[idx])\n",
        "  if idx == 0:\n",
        "    print(float(out_NN[idx]), float(out_LR[idx]), float(out_svr[idx]), score / 3)\n",
        "  score /= 3\n",
        "  out_ensemble.append(score)\n",
        "out_ensemble = np.array(out_ensemble)\n",
        "out_ensemble = out_ensemble.reshape((out_ensemble.shape[0], 1))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.16710498929023743 0.1179137668165319 0.11636303710497864 0.13379393107058266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpm-T76tA6Sy",
        "outputId": "8ef270c8-3db0-4662-d847-ff89b18b3472"
      },
      "source": [
        "evaluate_metrics(out_ensemble, Y_test_single_np)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peason's R: 0.7381368031461403\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7105119840090347, pvalue=8.322838188474213e-142)\n",
            "R Squared: 0.5448459401588038\n",
            "MSE: 0.007398631813890535\n",
            "MAE: 0.06638338984666789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqKDnQLiBCb-"
      },
      "source": [
        "out_ensemble_multi = []\n",
        "\n",
        "for idx in range(len(out_NN_multi)):\n",
        "  score = 0\n",
        "  score += float(out_NN_multi[idx])\n",
        "  score += float(out_LR_multi[idx])\n",
        "  score += float(out_svr_multi[idx])\n",
        "  score /= 3\n",
        "  out_ensemble_multi.append(score)\n",
        "out_ensemble_multi = np.array(out_ensemble_multi)\n",
        "out_ensemble_multi = out_ensemble_multi.reshape((out_ensemble_multi.shape[0], 1))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWxIKvOqBlAj",
        "outputId": "dbccae1b-e873-4b31-823a-482d8bbc3e04"
      },
      "source": [
        "evaluate_metrics(out_ensemble_multi, Y_test_multi_np)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peason's R: 0.7881390769343021\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7706449522570928, pvalue=1.8581243837015525e-37)\n",
            "R Squared: 0.6211632045908537\n",
            "MSE: 0.009589619324499697\n",
            "MAE: 0.07759087966511614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-AlPjA09vUO"
      },
      "source": [
        "# path_out_single = '/content/gdrive/My Drive/NLP_Outputs/out_single.csv'\n",
        "# path_out_multi = '/content/gdrive/My Drive/NLP_Outputs/out_multi.csv'"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FygFmc_SOx61"
      },
      "source": [
        "# with open(path_out_single, 'w') as f:\n",
        "#   writer = csv.writer(f)\n",
        "#   for idx in range(len(out_ensemble)):\n",
        "#     writer.writerow([single_ids[idx], float(out_ensemble[idx])])\n",
        "\n",
        "# with open(path_out_multi, 'w') as f:\n",
        "#   writer = csv.writer(f)\n",
        "#   for idx in range(len(out_ensemble_multi)):\n",
        "#     writer.writerow([multi_ids[idx], float(out_ensemble_multi[idx])])"
      ],
      "execution_count": 70,
      "outputs": []
    }
  ]
}