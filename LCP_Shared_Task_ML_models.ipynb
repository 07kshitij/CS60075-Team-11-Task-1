{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LCP_Shared_Task_ML_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+ReftWdoezxU/to3exmf5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/07kshitij/CS60075-Team-11-Task-1/blob/main/LCP_Shared_Task_ML_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnSmpv2kroCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f89231-2af9-4c24-9622-70e3eb4a4d3e"
      },
      "source": [
        "!pip install spacy-syllables\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip3 install wordfreq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy-syllables in /usr/local/lib/python3.7/dist-packages (3.0.1)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy-syllables) (3.0.5)\n",
            "Requirement already satisfied: pyphen<0.11.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from spacy-syllables) (0.10.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.11.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (54.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.23.0)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (1.7.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (4.41.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (8.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (0.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (0.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.7.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (20.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (1.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.3->spacy-syllables) (1.1.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (1.24.3)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.0.3->spacy-syllables) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<4.0.0,>=3.0.3->spacy-syllables) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (2.4.7)\n",
            "2021-04-07 13:18:53.161943: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Requirement already satisfied: en-core-web-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl#egg=en_core_web_sm==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.8.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.7/dist-packages (2.4.2)\n",
            "Requirement already satisfied: regex>=2020.04.04 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (2021.4.4)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (3.1.0)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (1.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv10GoDvxaeM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from wordfreq import word_frequency\n",
        "from scipy import stats\n",
        "import csv\n",
        "import spacy\n",
        "from spacy_syllables import SpacySyllables"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d73e77vIu_k7",
        "outputId": "04f85092-8bf2-4f7b-fc21-69200f35fdf1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDhaYmV9ULB7",
        "outputId": "a1e47317-e639-4e0b-92ce-a786b45ac2c5"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-07 13:19:04--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-04-07 13:19:05--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-04-07 13:19:05--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.05MB/s    in 2m 40s  \n",
            "\n",
            "2021-04-07 13:21:45 (5.13 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeSBDTRwUl6o",
        "outputId": "4d022e76-16a5-4182-9f08-99a0431ef55d"
      },
      "source": [
        "!unzip glove*.zip\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "gdrive\t\t   glove.6B.200d.txt  glove.6B.50d.txt\tglove.6B.zip.1\n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\tsample_data\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrgVK4JYrsm5"
      },
      "source": [
        "# Seed all rngs for deterministic results\n",
        "def seed_all(seed = 0):\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw8rWJgy0Y7Y"
      },
      "source": [
        "seed_all(0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5RpElbs_7jn"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I-7XsrzA_uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b4d5d5-d39b-4c31-d2af-1cbebb926779"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"syllables\", after='tagger') # Add the syllable tagger pipe"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy_syllables.SpacySyllables at 0x7fe71fc5f250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPyP8nfVAxIw"
      },
      "source": [
        "SINGLE_TRAIN_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/train/lcp_single_train.tsv\"\n",
        "SINGLE_TEST_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/test-labels/lcp_single_test.tsv\"\n",
        "\n",
        "MULTI_TRAIN_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/train/lcp_multi_train.tsv\"\n",
        "MULTI_TEST_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/test-labels/lcp_multi_test.tsv\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZQb1lbDA9o2"
      },
      "source": [
        "def get_data_frames():\n",
        "  df_train_single = pd.read_csv(SINGLE_TRAIN_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "  df_test_single = pd.read_csv(SINGLE_TEST_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "\n",
        "  df_train_multi = pd.read_csv(MULTI_TRAIN_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "  df_test_multi = pd.read_csv(MULTI_TEST_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "\n",
        "  return df_train_single, df_test_single, df_train_multi, df_test_multi"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLkKx7GsyGcK"
      },
      "source": [
        "df_train_single, df_test_single, df_train_multi, df_test_multi = get_data_frames()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYq6zO2NPEBt"
      },
      "source": [
        "Features used \n",
        "\n",
        "* Word Embedding [GloVe 50 dimensional embeddings](http://nlp.stanford.edu/data/glove.6B.zip)\n",
        "* Length of word\n",
        "* Syllable count [PyPy](https://pypi.org/project/syllables/)\n",
        "* Word Frequency [PyPy](https://pypi.org/project/wordfreq/)\n",
        "* POS tag [Spacy](https://spacy.io/usage/linguistic-features#pos-tagging)\n",
        "\n",
        "[Reference](https://www.aclweb.org/anthology/W18-0508.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DOS3snOBoin"
      },
      "source": [
        "single_tokens_train_raw = df_train_single[\"token\"].astype(str).to_list()\n",
        "single_tokens_test_raw = df_test_single[\"token\"].astype(str).to_list()\n",
        "\n",
        "y_single_train = df_train_single[\"complexity\"].astype(np.float32).to_numpy()\n",
        "y_single_test = df_test_single[\"complexity\"].astype(np.float32).to_numpy()\n",
        "\n",
        "multi_tokens_train_raw = df_train_multi[\"token\"].astype(str).to_list()\n",
        "multi_tokens_test_raw = df_test_multi[\"token\"].astype(str).to_list()\n",
        "\n",
        "y_multi_train = df_train_multi[\"complexity\"].astype(np.float32).to_numpy()\n",
        "y_multi_test = df_test_multi[\"complexity\"].astype(np.float32).to_numpy()\n",
        "\n",
        "sent_train_single_raw = df_train_single[\"sentence\"].to_list()\n",
        "sent_test_single_raw = df_test_single[\"sentence\"].to_list()\n",
        "\n",
        "sent_train_multi_raw = df_train_multi[\"sentence\"].to_list()\n",
        "sent_test_multi_raw = df_test_multi[\"sentence\"].to_list()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z00nnMqBVTmi"
      },
      "source": [
        "EMBEDDING_DIM = 50\n",
        "\n",
        "def get_embeddings():\n",
        "  embedding_index = {}\n",
        "  with open('glove.6B.{}d.txt'.format(EMBEDDING_DIM), 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      token = values[0]\n",
        "      embedding_index[token] = np.asarray(values[1:], dtype='float32')\n",
        "  return embedding_index"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcpQP9sYzfun",
        "outputId": "ad15a649-7dd6-433f-b826-36b583cbafe8"
      },
      "source": [
        "embedding_index = get_embeddings()\n",
        "print('Token count in embeddings: {}'.format(len(embedding_index)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token count in embeddings: 400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXTRtrC3ptjk"
      },
      "source": [
        "biLSTM to predict target probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eykntycSwUQO"
      },
      "source": [
        "HIDDEN_DIM = 10"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOcp3gYaRFyv"
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "  seq = seq.split()\n",
        "  idxs = [to_ix[w.lower()] if w.lower() in to_ix else len(to_ix) for w in seq]\n",
        "  idxs = torch.tensor(idxs)\n",
        "  idxs = nn.functional.one_hot(idxs, num_classes=len(to_ix))\n",
        "  idxs = torch.tensor(idxs, dtype=torch.float32)\n",
        "  return idxs\n",
        "\n",
        "\n",
        "def map_token_to_idx():\n",
        "  word_to_ix = {}\n",
        "  word_to_ix_multi = {}\n",
        "  for sent in sent_train_single_raw:\n",
        "    sent = sent.split()\n",
        "    for word in sent:\n",
        "      word = word.lower()\n",
        "      if word not in word_to_ix:\n",
        "        word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "  for sent in sent_train_multi_raw:\n",
        "    sent = sent.split()\n",
        "    for word in sent:\n",
        "      word = word.lower()\n",
        "      if word not in word_to_ix_multi:\n",
        "        word_to_ix_multi[word] = len(word_to_ix_multi)\n",
        "  \n",
        "  return word_to_ix, word_to_ix_multi"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Td2vdR0JqZ",
        "outputId": "715643ab-705e-4ebd-981d-bc905b5bff97"
      },
      "source": [
        "word_to_ix, word_to_ix_multi = map_token_to_idx()\n",
        "print('SWE vocab size: {}\\nMWE vocab size: {}'.format(len(word_to_ix), len(word_to_ix_multi)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SWE vocab size: 24350\n",
            "MWE vocab size: 9699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOFJH3_us5bj"
      },
      "source": [
        "biLSTM to calculate token probability given context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8rD4jq599bT"
      },
      "source": [
        "class biLSTM(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size):\n",
        "    super(biLSTM, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "    self.hidden2tag = nn.Linear(2 * hidden_dim, output_size)\n",
        "\n",
        "  def prepare_embedding(self, sentence):\n",
        "    embeddings = []\n",
        "    for word in sentence:\n",
        "      word = word.lower()\n",
        "      if word in embedding_index:\n",
        "        embeddings.extend(embedding_index[word])\n",
        "      else:\n",
        "        embeddings.extend(np.random.random(EMBEDDING_DIM).tolist())\n",
        "    embeddings = torch.tensor(embeddings, dtype=torch.float32, device=device)\n",
        "    return embeddings\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    sentence = sentence.split()\n",
        "    embeds = self.prepare_embedding(sentence)\n",
        "    lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "    tag_scores = F.softmax(tag_space, dim=1)\n",
        "    return tag_scores"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrolGYkytD4f"
      },
      "source": [
        "biLSTM model for single word targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8of3r04B5Tn"
      },
      "source": [
        "model = biLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(word_to_ix))\n",
        "path_biLSTM_single = '/content/gdrive/My Drive/TrainedModels/biLSTM.pt'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCAl7MWG4gqN"
      },
      "source": [
        "USE_PRETRAINED_SINGLE_WORD_TARGET_MODEL = True"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DfKvyAUT8VN",
        "outputId": "1b2f30c8-cfc2-4f62-f425-0fb5e3a254de"
      },
      "source": [
        "if USE_PRETRAINED_SINGLE_WORD_TARGET_MODEL:\n",
        "  print('Using pre-trained biLSTM on single target expressions')\n",
        "  model = torch.load(path_biLSTM_single)\n",
        "  model.eval()\n",
        "else:\n",
        "  print('Training biLSTM on single target expressions')\n",
        "  # Train the model for 10 epochs\n",
        "  loss_function = nn.MSELoss()n\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "  for epoch in range(10):\n",
        "    loss_sum = 0\n",
        "    for sentence in sent_train_single_raw:\n",
        "      model.zero_grad()\n",
        "      targets = prepare_sequence(sentence, word_to_ix)\n",
        "      tag_scores = model(sentence)\n",
        "      loss = loss_function(tag_scores, targets)\n",
        "      loss_sum += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print('Epoch: {} Loss: {}'.format(epoch, loss_sum.item()))\n",
        "  # path_biLSTM_single = '/content/gdrive/My Drive/TrainedModels/biLSTM.pt'\n",
        "  # torch.save(model, path_biLSTM_single)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pre-trained biLSTM on single target expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYMs37iTtLDn"
      },
      "source": [
        "biLSTM model for multi word targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1Ypxmd65_GL"
      },
      "source": [
        "model_multi = biLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix_multi), len(word_to_ix_multi))\n",
        "path_biLSTM_multi = '/content/gdrive/My Drive/TrainedModels/biLSTM_multi.pt'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSw5V4vi6Hx0"
      },
      "source": [
        "USE_PRETRAINED_MULTI_WORD_TARGET_MODEL = True"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2QK4FDT6eL0",
        "outputId": "cb6aeb27-a32a-417e-87ae-6fabf47a742d"
      },
      "source": [
        "if USE_PRETRAINED_MULTI_WORD_TARGET_MODEL:\n",
        "  print('Using pre-trained biLSTM on multi target expressions')\n",
        "  model_multi = torch.load(path_biLSTM_multi)\n",
        "  model_multi.eval()\n",
        "else:\n",
        "  print('Training biLSTM on multi target expressions')\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model_multi.parameters(), lr=0.01)\n",
        "  for epoch in range(10):\n",
        "    loss_sum = 0\n",
        "    for sentence in sent_train_multi_raw:\n",
        "      model_multi.zero_grad()\n",
        "      targets = prepare_sequence(sentence, word_to_ix_multi)\n",
        "      tag_scores = model_multi(sentence)\n",
        "      loss = loss_function(tag_scores, targets)\n",
        "      loss_sum += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print('Epoch: {} Loss: {}'.format(epoch, loss_sum.item()))\n",
        "  # path_biLSTM_multi = '/content/gdrive/My Drive/TrainedModels/biLSTM_multi.pt'\n",
        "  # torch.save(model_multi, path_biLSTM_multi)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pre-trained biLSTM on multi target expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5PUts_pSloU"
      },
      "source": [
        "def prepare_features_single_word(tokens, sentences):\n",
        "  features = []\n",
        "  for idx, word in enumerate(tokens):\n",
        "    word = word.lower()\n",
        "    feature = []\n",
        "\n",
        "    # Word length\n",
        "    feature.append(len(word))\n",
        "    doc = nlp(word)\n",
        "\n",
        "    # Syllable count and word frequency in the corpus\n",
        "    # Spacy tokenizes the input sentence\n",
        "    # In this case we would have only one token, the target word\n",
        "    for token in doc:\n",
        "      feature.append(token._.syllables_count)\n",
        "      feature.append(word_frequency(word, 'en'))\n",
        "\n",
        "    # Probability of target word `word` in the sentence estimated from by `model`\n",
        "    if word in word_to_ix:\n",
        "      # Output scores for each of the word in the sentence\n",
        "      out = model(sentences[idx])\n",
        "      pos = -1\n",
        "      for itr, token in enumerate(sentences[idx].split()):\n",
        "        if token.lower() == word:\n",
        "          pos = itr\n",
        "          break\n",
        "      id_pos = word_to_ix[word] # word to id mapping\n",
        "      feature.append(float(out[pos][id_pos]))\n",
        "    else:\n",
        "      # `word` not in vocabulary, so cannot predict probability in context\n",
        "      feature.append(0.0)\n",
        "\n",
        "    # GloVE embedding for the `word`\n",
        "    if word in embedding_index:\n",
        "      feature.extend(embedding_index[word].tolist())\n",
        "    else:\n",
        "      # `word` not in the GloVE corpus, take a random embedding\n",
        "      feature.extend(np.random.random(EMBEDDING_DIM).tolist())\n",
        "    features.append(feature)\n",
        "\n",
        "    if (idx + 1) % 500 == 0:\n",
        "      print('Prepared features for {} single target word sentences'.format(idx + 1))\n",
        "  return features"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BY3iu53udY"
      },
      "source": [
        "def prepare_features_multi_word(tokens, sentences):\n",
        "  features = []\n",
        "  for idx, word in enumerate(tokens):\n",
        "    word = word.lower()\n",
        "    feature = []\n",
        "    doc = nlp(word)\n",
        "    word = word.split(' ')\n",
        "    assert(len(word) == 2)\n",
        "\n",
        "    # MWE length = sum(length of individual words)\n",
        "    feature.append(len(word[0]) + len(word[1]))\n",
        "\n",
        "    syllables = 0\n",
        "    probability = 1\n",
        "    embedding = np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "    # Syllable count and word frequency in the corpus\n",
        "    # Spacy tokenizes the input sentence\n",
        "    # In this case we would have two tokens\n",
        "\n",
        "    for token in doc:\n",
        "      word_ = token.text\n",
        "      syllables += token._.syllables_count\n",
        "      probability *= word_frequency(word_, 'en')\n",
        "\n",
        "      # GloVE embedding current `word_` of the MWE\n",
        "      if word_ in embedding_index:\n",
        "        embedding = embedding + embedding_index[word_]\n",
        "      else:\n",
        "        # `word_` not in the GloVE corpus, take a random embedding\n",
        "        embedding = embedding + np.random.random(EMBEDDING_DIM)\n",
        "\n",
        "    # Average embedding of the two tokens in the MWE\n",
        "    embedding = embedding / 2\n",
        "    feature.append(syllables)\n",
        "    feature.append(probability)\n",
        "\n",
        "    # Product of probabilities of constituent words in the MWE\n",
        "    if word[0] in word_to_ix_multi and word[1] in word_to_ix_multi:\n",
        "      # Output scores for each of the word in the sentence\n",
        "      out = model_multi(sentences[idx])\n",
        "      pos0, pos1 = -1, -1\n",
        "      for itr, token in enumerate(sentences[idx].split()):\n",
        "        if token.lower() == word[0]:\n",
        "          pos0 = itr\n",
        "          pos1 = itr + 1\n",
        "          break\n",
        "      id_pos0 = word_to_ix_multi[word[0]]\n",
        "      id_pos1 = word_to_ix_multi[word[1]]\n",
        "      feature.append(float(out[pos0][id_pos0] * out[pos1][id_pos1]))\n",
        "    else:\n",
        "      # Either of the constituent words of the MWE not in vocabulary \\\n",
        "      # So cannot predict probability in context\n",
        "      feature.append(0.0)\n",
        "\n",
        "    feature.extend(embedding.tolist())\n",
        "    features.append(feature)\n",
        "\n",
        "    if (idx + 1) % 500 == 0:\n",
        "      print('Prepared features for {} multi target word sentences'.format(idx + 1))\n",
        "\n",
        "  return features"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYuU10C8hZjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07fc954-5f24-4343-a955-d9cd84addd8f"
      },
      "source": [
        "print('+++ Generating Train features for Single word expressions +++')\n",
        "features_train_single = prepare_features_single_word(single_tokens_train_raw, sent_train_single_raw)\n",
        "print('+++ Generating Test features for Single word expressions +++')\n",
        "features_test_single = prepare_features_single_word(single_tokens_test_raw, sent_test_single_raw)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ Generating Train features for Single word expressions +++\n",
            "Prepared features for 500 single target word sentences\n",
            "Prepared features for 1000 single target word sentences\n",
            "Prepared features for 1500 single target word sentences\n",
            "Prepared features for 2000 single target word sentences\n",
            "Prepared features for 2500 single target word sentences\n",
            "Prepared features for 3000 single target word sentences\n",
            "Prepared features for 3500 single target word sentences\n",
            "Prepared features for 4000 single target word sentences\n",
            "Prepared features for 4500 single target word sentences\n",
            "Prepared features for 5000 single target word sentences\n",
            "Prepared features for 5500 single target word sentences\n",
            "Prepared features for 6000 single target word sentences\n",
            "Prepared features for 6500 single target word sentences\n",
            "Prepared features for 7000 single target word sentences\n",
            "Prepared features for 7500 single target word sentences\n",
            "+++ Generating Test features for Single word expressions +++\n",
            "Prepared features for 500 single target word sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqj-99dkA43o",
        "outputId": "28cd9347-9772-45b8-cf59-1b8e296f2cc8"
      },
      "source": [
        "print('+++ Generating Train features for Multi word expressions +++')\n",
        "features_train_multi = prepare_features_multi_word(multi_tokens_train_raw, sent_train_multi_raw)\n",
        "print('+++ Generating Test features for Multi word expressions +++')\n",
        "features_test_multi = prepare_features_multi_word(multi_tokens_test_raw, sent_test_multi_raw)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+++ Generating Train features for Multi word expressions +++\n",
            "Prepared features for 500 multi target word sentences\n",
            "Prepared features for 1000 multi target word sentences\n",
            "Prepared features for 1500 multi target word sentences\n",
            "+++ Generating Test features for Multi word expressions +++\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESn2VJdU1p8h"
      },
      "source": [
        "# Convert all features to torch.tensor to enable use in PyTorch models\n",
        "X_train_single_tensor = torch.tensor(features_train_single, dtype=torch.float32, device=device)\n",
        "X_test_single_tensor = torch.tensor(features_test_single, dtype=torch.float32, device=device)\n",
        "X_train_multi_tensor = torch.tensor(features_train_multi, dtype=torch.float32, device=device)\n",
        "X_test_multi_tensor = torch.tensor(features_test_multi, dtype=torch.float32, device=device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VikNi2ZmtEn"
      },
      "source": [
        "# Reshape all output complexity scores to single dimension vectors\n",
        "y_single_train = y_single_train.reshape(y_single_train.shape[0], -1)\n",
        "y_single_test = y_single_test.reshape(y_single_test.shape[0], -1)\n",
        "y_multi_train = y_multi_train.reshape(y_multi_train.shape[0], -1)\n",
        "y_multi_test = y_multi_test.reshape(y_multi_test.shape[0], -1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhjYgCiA13Z1"
      },
      "source": [
        "# Convert all target outputs to torch.tensor to enable use in PyTorch models\n",
        "Y_train_single_tensor = torch.tensor(y_single_train, dtype=torch.float32, device=device)\n",
        "Y_test_single_tensor = torch.tensor(y_single_test, dtype=torch.float32, device=device)\n",
        "Y_train_multi_tensor = torch.tensor(y_multi_train, dtype=torch.float32, device=device)\n",
        "Y_test_multi_tensor = torch.tensor(y_multi_test, dtype=torch.float32, device=device)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSRoIvSU2McF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "075d9e4e-da44-4339-a04f-32782b67d805"
      },
      "source": [
        "# Ensure each sample from test and train for single word expression is taken\n",
        "print(X_train_single_tensor.shape)\n",
        "print(X_test_single_tensor.shape)\n",
        "print(Y_train_single_tensor.shape)\n",
        "print(Y_test_single_tensor.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([7662, 54])\n",
            "torch.Size([917, 54])\n",
            "torch.Size([7662, 1])\n",
            "torch.Size([917, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7de9O0tZBvqD",
        "outputId": "14448f2c-0005-429f-f5f9-4ddc97c93c95"
      },
      "source": [
        "# Ensure each sample from test and train for multi word expression is taken\n",
        "print(X_train_multi_tensor.shape)\n",
        "print(X_test_multi_tensor.shape)\n",
        "print(Y_train_multi_tensor.shape)\n",
        "print(Y_test_multi_tensor.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1517, 54])\n",
            "torch.Size([184, 54])\n",
            "torch.Size([1517, 1])\n",
            "torch.Size([184, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONmt2TBCK9jO"
      },
      "source": [
        "def convert_tensor_to_np(y):\n",
        "  if device == torch.device(\"cuda\"):\n",
        "    y = y.cpu()\n",
        "  y = y.detach().numpy()\n",
        "  return y"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m07BB6hLc70"
      },
      "source": [
        "from copy import deepcopy"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QVP68p13j5C"
      },
      "source": [
        "# Evaluate the metrics upon which the model would be evaluated\n",
        "def evaluate_metrics(labels, predicted):\n",
        "  vx, vy = [], []\n",
        "  if torch.is_tensor(labels):\n",
        "    vx = labels.clone()\n",
        "    vx = convert_tensor_to_np(vx)\n",
        "  else:\n",
        "    vx = deepcopy(labels)\n",
        "  if torch.is_tensor(predicted):\n",
        "    vy = predicted.clone()\n",
        "    vy = convert_tensor_to_np(vy)\n",
        "  else:\n",
        "    vy = deepcopy(predicted)\n",
        "\n",
        "  pearsonR = np.corrcoef(vx.T, vy.T)[0, 1]\n",
        "  spearmanRho = stats.spearmanr(vx, vy)\n",
        "  MSE = np.mean((vx - vy) ** 2)\n",
        "  MAE = np.mean(np.absolute(vx - vy))\n",
        "  RSquared = pearsonR ** 2\n",
        "\n",
        "  print(\"Peason's R: {}\".format(pearsonR))\n",
        "  print(\"Spearman's rho: {}\".format(spearmanRho))\n",
        "  print(\"R Squared: {}\".format(RSquared))\n",
        "  print(\"MSE: {}\".format(MSE))\n",
        "  print(\"MAE: {}\".format(MAE))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGBqpNE_tUWK"
      },
      "source": [
        "## Neural Network\n",
        "\n",
        "* $N$ input sentences  \n",
        "\n",
        "* d (=EMBEDDING_DIM) word embedding\n",
        "\n",
        "* $I$ = Word Embedding matrix ($N \\times d$)\n",
        "\n",
        "* $W_1, W_2, W_3, W_4 := (d \\times 256), (256 \\times 128), (128 \\times 64), (64 \\times 1)$\n",
        "\n",
        "* Equations\n",
        "\n",
        "  * $o_1 = tanh(I \\times W_1 + b_1)$\n",
        "\n",
        "  * $o_2 = tanh(o_1 \\times W_2 + b_2)$\n",
        "\n",
        "  * $o_3 = tanh(o_2 \\times W_3 + b_3)$\n",
        "\n",
        "  * $o_4 = \\sigma(o_3 \\times W_4)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLhlNCoX2UrA"
      },
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(embedding_dim, 256, bias=True)\n",
        "    self.linear2 = nn.Linear(256, 128, bias=True)\n",
        "    self.linear3 = nn.Linear(128, 64, bias=True)\n",
        "    self.linear4 = nn.Linear(64, 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = torch.tanh(self.linear1(input))\n",
        "    out = torch.tanh(self.linear2(out))\n",
        "    out = torch.tanh(self.linear3(out))\n",
        "    out = torch.sigmoid(self.linear4(out))\n",
        "    return out"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwSTHAPd28r8"
      },
      "source": [
        "loss_function = nn.MSELoss()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAW0QJDkCWvv"
      },
      "source": [
        "embedding_dim = X_train_single_tensor.shape[1]\n",
        "model_NN = NN(embedding_dim)\n",
        "model_NN.to(device)\n",
        "path_NN = '/content/gdrive/My Drive/TrainedModels/NN_0.731.pt'"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lUaRjKGCkoJ"
      },
      "source": [
        "USE_PRETRAINED_SINGLE_WORD_TARGET_NN = True"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVCTc8Zf3KoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df528d57-3f04-41f7-c542-b77e26212c4b"
      },
      "source": [
        "if USE_PRETRAINED_SINGLE_WORD_TARGET_NN:\n",
        "  print('Using pre-trained NN on single target expressions')\n",
        "  model_NN = torch.load(path_NN)\n",
        "  model_NN.eval()\n",
        "else:\n",
        "  print('Training NN on single target expressions...')\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model_NN.parameters(), lr=0.001)\n",
        "  for epoch in range(30):\n",
        "    optimizer.zero_grad()\n",
        "    out = model_NN(X_train_single_tensor)\n",
        "    loss = loss_function(out, Y_train_single_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Epoch {} : {}\".format(epoch + 1, loss.item()))\n",
        "  # path_NN = '/content/gdrive/My Drive/TrainedModels/NN_0.731.pt'\n",
        "  # torch.save(model_NN, path_NN)  "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pre-trained NN on single target expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DMsEM5R3cUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3454005-573c-4778-da31-28ee52544905"
      },
      "source": [
        "out_NN = model_NN(X_test_single_tensor)\n",
        "evaluate_metrics(out_NN, Y_test_single_tensor)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peason's R: 0.7307197818006539\n",
            "Spearman's rho: SpearmanrResult(correlation=0.6897209934020212, pvalue=1.9230869914158295e-130)\n",
            "R Squared: 0.5339513995147952\n",
            "MSE: 0.00774401007220149\n",
            "MAE: 0.06808073073625565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YOGT0TCELKo"
      },
      "source": [
        "embedding_dim = X_train_multi_tensor.shape[1]\n",
        "model_NN_multi = NN(embedding_dim)\n",
        "model_NN_multi.to(device)\n",
        "path_NN_multi = '/content/gdrive/My Drive/TrainedModels/NN_multi_0.774.pt'"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNZIIOkfDM7E"
      },
      "source": [
        "USE_PRETRAINED_MULTI_WORD_TARGET_NN = True"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl6L4RlYB-Wj",
        "outputId": "7e3e268b-408d-4de9-b301-c9635f4ab726"
      },
      "source": [
        "if USE_PRETRAINED_MULTI_WORD_TARGET_NN:\n",
        "  print('Using pre-trained NN on multi target expressions')\n",
        "  model_NN_multi = torch.load(path_NN_multi)\n",
        "  model_NN_multi.eval()\n",
        "else:\n",
        "  print('Training NN on multi target expressions...')\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model_NN_multi.parameters(), lr=0.001)\n",
        "  for epoch in range(30):\n",
        "    optimizer.zero_grad()\n",
        "    out = model_NN_multi(X_train_multi_tensor)\n",
        "    loss = loss_function(out, Y_train_multi_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Epoch {} : {}\".format(epoch + 1, loss.item()))\n",
        "  # path_NN_multi = '/content/gdrive/My Drive/TrainedModels/NN_multi_0.774.pt'\n",
        "  # torch.save(model_NN_multi, path_NN_multi)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pre-trained NN on multi target expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msBIen6KCGY2",
        "outputId": "be5bf0db-6ab3-4426-e9e4-7ec3f1980ee3"
      },
      "source": [
        "out_NN_multi = model_NN_multi(X_test_multi_tensor)\n",
        "evaluate_metrics(out_NN_multi, Y_test_multi_tensor)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peason's R: 0.7742502832762863\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7718377190311184, pvalue=1.2273362586858998e-37)\n",
            "R Squared: 0.5994635011534096\n",
            "MSE: 0.009973191656172276\n",
            "MAE: 0.07805463671684265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SpGwzvfAQ0y"
      },
      "source": [
        "## Machine Learning Methods\n",
        "\n",
        "* Linear Regression\n",
        "\n",
        "* Support Vector Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8RRwfzp0R-K"
      },
      "source": [
        "X_train_single_np = np.array(features_train_single)\n",
        "X_test_single_np = np.array(features_test_single)\n",
        "Y_train_single_np = np.array(y_single_train.reshape(y_single_train.shape[0], -1))\n",
        "Y_test_single_np = np.array(y_single_test.reshape(y_single_test.shape[0], -1))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2LzJmZwoFMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda865d3-b9ea-4846-c17e-a7ce5624f83f"
      },
      "source": [
        "print(X_train_single_np.shape)\n",
        "print(X_test_single_np.shape)\n",
        "print(Y_train_single_np.shape)\n",
        "print(Y_test_single_np.shape)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7662, 54)\n",
            "(917, 54)\n",
            "(7662, 1)\n",
            "(917, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AW_YHxS73xW"
      },
      "source": [
        "X_train_multi_np = np.array(features_train_multi)\n",
        "X_test_multi_np = np.array(features_test_multi)\n",
        "Y_train_multi_np = np.array(y_multi_train.reshape(y_multi_train.shape[0], -1))\n",
        "Y_test_multi_np = np.array(y_multi_test.reshape(y_multi_test.shape[0], -1))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQP30U5JZ6Qi",
        "outputId": "54b5bf66-2841-4385-c35e-aa145ac3cb46"
      },
      "source": [
        "print(X_train_multi_np.shape)\n",
        "print(X_test_multi_np.shape)\n",
        "print(Y_train_multi_np.shape)\n",
        "print(Y_test_multi_np.shape)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1517, 54)\n",
            "(184, 54)\n",
            "(1517, 1)\n",
            "(184, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwsHCYnr3D3E"
      },
      "source": [
        "### Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ATmatOhioJI"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPRzjdJtOWlM"
      },
      "source": [
        "def evaluateLinearRegression(X_train, Y_train, X_test, Y_test):\n",
        "  reg = LinearRegression().fit(X_train, Y_train)\n",
        "  out = reg.predict(X_test)\n",
        "  evaluate_metrics(out, Y_test)\n",
        "  return out"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMSTucrSOpk7",
        "outputId": "67a847ea-4697-4ce3-f7c8-aeabec5829a1"
      },
      "source": [
        "print('Linear Regression for Single word expressions')\n",
        "out_LR = evaluateLinearRegression(X_train_single_np, Y_train_single_np, X_test_single_np, Y_test_single_np)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression for Single word expressions\n",
            "Peason's R: 0.7108434116187762\n",
            "Spearman's rho: SpearmanrResult(correlation=0.6918562248147357, pvalue=1.4505768829498638e-131)\n",
            "R Squared: 0.5052983558418208\n",
            "MSE: 0.008010032054359562\n",
            "MAE: 0.06855075784165758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoDpEh5ZPA6B",
        "outputId": "b63e1ca8-f515-4962-e4be-4b6e96e870bb"
      },
      "source": [
        "print('Linear Regression for Multi word expressions')\n",
        "out_LR_multi = evaluateLinearRegression(X_train_multi_np, Y_train_multi_np, X_test_multi_np, Y_test_multi_np)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression for Multi word expressions\n",
            "Peason's R: 0.7672485716941783\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7573872177422218, pvalue=1.5861218509138286e-35)\n",
            "R Squared: 0.5886703707667565\n",
            "MSE: 0.009940570799946988\n",
            "MAE: 0.0800150507072244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot7P6ba-3Gwx"
      },
      "source": [
        "### Support Vector Regressor\n",
        "\n",
        "* Radial basis function\n",
        "* C = 0.05\n",
        "* epsilon = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I21y5EAG2BHY"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyTiLWD33PH1"
      },
      "source": [
        "def evaluateSVR(X_train, Y_train, X_test, Y_test):\n",
        "  svr = make_pipeline(StandardScaler(), SVR(C=0.05, epsilon=0.01))\n",
        "  svr.fit(X_train, Y_train.reshape(-1))\n",
        "  out = svr.predict(X_test)\n",
        "  evaluate_metrics(out, Y_test)\n",
        "  return out"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffGoiNKq3hBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02acb703-dd26-474e-c400-94390e68a84d"
      },
      "source": [
        "print('SVR for Single word expressions')\n",
        "out_svr = evaluateSVR(X_train_single_np, Y_train_single_np, X_test_single_np, Y_test_single_np)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVR for Single word expressions\n",
            "Peason's R: 0.732967844251433\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7037429683169284, pvalue=5.357912593044118e-138)\n",
            "R Squared: 0.537241860706593\n",
            "MSE: 0.023820367087864787\n",
            "MAE: 0.1193907331118814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWFRhtSEPl9t",
        "outputId": "a4f0ad87-063d-4dc8-b5a9-12c4561f0c8c"
      },
      "source": [
        "print('SVR for Multi word expressions')\n",
        "out_svr_multi = evaluateSVR(X_train_multi_np, Y_train_multi_np, X_test_multi_np, Y_test_multi_np)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVR for Multi word expressions\n",
            "Peason's R: 0.7834043625882116\n",
            "Spearman's rho: SpearmanrResult(correlation=0.754343227255628, pvalue=4.2297035272986037e-35)\n",
            "R Squared: 0.613722395322242\n",
            "MSE: 0.03332096626418293\n",
            "MAE: 0.14654354850969184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhJgiYbCCpJf"
      },
      "source": [
        "### Summary \n",
        "\n",
        "\n",
        "| Model | Type | Pearson | MSE | MAE|\n",
        "|---|---|---|---|---|\n",
        "|Neural Network| Single word| 0.7307 | 0.0077 | 0.0681|\n",
        "|Neural Network| Multi word| 0.7742 | 0.0100 | 0.0780|\n",
        "|Linear Regression| Single word| 0.7108 | 0.0080 | 0.0685|\n",
        "|Linear Regression| Multi word| 0.7672 | 0.0099 | 0.0800|\n",
        "|SVR| Single word| 0.7330 | 0.0238 | 0.1194|\n",
        "|SVR| Multi word| 0.7834 | 0.0333 | 0.1465|\n",
        "\n"
      ]
    }
  ]
}