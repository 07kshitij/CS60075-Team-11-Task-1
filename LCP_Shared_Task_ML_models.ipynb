{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LCP_Shared_Task_ML_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3AU9Fd4OsmbhB+3k1GvbT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/07kshitij/CS60075-Team-11-Task-1/blob/main/LCP_Shared_Task_ML_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnSmpv2kroCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8c2d38-3035-4b46-85bc-e59a52ca15dc"
      },
      "source": [
        "!pip install spacy-syllables\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip3 install wordfreq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy-syllables\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/71/0f9ea26326aba87950968177577da72e634773e49366eec0f023a7d88a1b/spacy_syllables-3.0.1-py3-none-any.whl\n",
            "Collecting spacy<4.0.0,>=3.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/70/a0b8bd0cb54d8739ba4d6fb3458785c3b9b812b7fbe93b0f10beb1a53ada/spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 15.6MB/s \n",
            "\u001b[?25hCollecting pyphen<0.11.0,>=0.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/5a/5bc036e01389bc6a6667a932bac3e388de6e7fa5777a6ff50e652f60ec79/Pyphen-0.10.0-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (1.0.5)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/5c/493a2f3bb0eac17b1d48129ecfd251f0520b6c89493e9fd0522f534a9e4a/catalogue-2.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (20.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.8.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (0.8.2)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/08/20e707519bcded1a0caa6fd024b767ac79e4e5d0fb92266bb7dcf735e338/thinc-8.0.2-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.0.5)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/54/76982427ceb495dd19ff982c966708c624b85e03c45bf1912feaf60c7b2d/srsly-2.4.0-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 51.2MB/s \n",
            "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (3.7.4.3)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/78/d8/e25bc7f99877de34def57d36769f0cce4e895b374cdc766718efc724f9ac/spacy_legacy-3.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (54.2.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (1.19.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.11.3)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (0.4.1)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.3->spacy-syllables) (2.0.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<4.0.0,>=3.0.3->spacy-syllables) (3.4.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.3->spacy-syllables) (1.1.1)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.3->spacy-syllables) (2.10)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=996c7915311655a5bdb342fba99b6e59c755741df0a8aadee4de44655aee9602\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: catalogue, srsly, pydantic, thinc, typer, spacy-legacy, smart-open, pathy, spacy, pyphen, spacy-syllables\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: smart-open 4.2.0\n",
            "    Uninstalling smart-open-4.2.0:\n",
            "      Successfully uninstalled smart-open-4.2.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.1 pathy-0.4.0 pydantic-1.7.3 pyphen-0.10.0 smart-open-3.0.0 spacy-3.0.5 spacy-legacy-3.0.2 spacy-syllables-3.0.1 srsly-2.4.0 thinc-8.0.2 typer-0.3.2\n",
            "2021-04-07 06:28:31.495548: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 326kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.8.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting wordfreq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/61/f069a02fd061175272ada29d2c472deaea777dd52445602b32333e8c3529/wordfreq-2.4.2.tar.gz (36.4MB)\n",
            "\u001b[K     |████████████████████████████████| 36.4MB 113kB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (1.0.2)\n",
            "Collecting langcodes>=3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/4f/70885b0b2afb5b092b465c6377d3c67847184e66a5c955881ba0b2902839/langcodes-3.1.0.tar.gz (168kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 44.2MB/s \n",
            "\u001b[?25hCollecting regex>=2020.04.04\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/28/5f08d8841013ccf72cd95dfff2500fe7fb39467af12c5e7b802d8381d811/regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720kB)\n",
            "\u001b[K     |████████████████████████████████| 727kB 38.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wordfreq, langcodes\n",
            "  Building wheel for wordfreq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordfreq: filename=wordfreq-2.4.2-cp37-none-any.whl size=36384934 sha256=33a92b27915a29a7fcf78d015145eb9e66b87f3eb6fae6ba61607d3f583c0052\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/7f/2f/ce0b9835ee84fc6b392b1e4563c77270c39a6882e74d22373b\n",
            "  Building wheel for langcodes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langcodes: filename=langcodes-3.1.0-cp37-none-any.whl size=165888 sha256=928dd7c381a6399df0a342812e46be381162e06fd4b1de038e0904d8cb4f9199\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/57/10/167a0f1c07409a7b72d8d08391fa9739ef37e834695575ccc8\n",
            "Successfully built wordfreq langcodes\n",
            "Installing collected packages: langcodes, regex, wordfreq\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed langcodes-3.1.0 regex-2021.4.4 wordfreq-2.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv10GoDvxaeM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from wordfreq import word_frequency\n",
        "from scipy import stats\n",
        "import csv\n",
        "import spacy\n",
        "from spacy_syllables import SpacySyllables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d73e77vIu_k7",
        "outputId": "6a3f0273-f087-4aac-b39e-8e6dca37ca7a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDhaYmV9ULB7",
        "outputId": "aa25007c-7626-4c95-c733-ea54eb4a33ae"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-07 06:29:18--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-04-07 06:29:19--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-04-07 06:29:19--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.07MB/s    in 2m 40s  \n",
            "\n",
            "2021-04-07 06:31:59 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeSBDTRwUl6o",
        "outputId": "3008fd1f-4b5a-4be0-8f7b-2c73d9df73c5"
      },
      "source": [
        "!unzip glove*.zip\n",
        "!ls\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrgVK4JYrsm5"
      },
      "source": [
        "# Seed all rngs for deterministic results\n",
        "def seed_all(seed = 0):\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw8rWJgy0Y7Y"
      },
      "source": [
        "seed_all(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5RpElbs_7jn"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I-7XsrzA_uB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ab5474-a48d-4364-d008-697c50631b53"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"syllables\", after='tagger') # Add the syllable tagger pipe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy_syllables.SpacySyllables at 0x7fbaf76a5bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPyP8nfVAxIw"
      },
      "source": [
        "SINGLE_TRAIN_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/train/lcp_single_train.tsv\"\n",
        "SINGLE_TEST_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/test-labels/lcp_single_test.tsv\"\n",
        "\n",
        "MULTI_TRAIN_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/train/lcp_multi_train.tsv\"\n",
        "MULTI_TEST_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/test-labels/lcp_multi_test.tsv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZQb1lbDA9o2"
      },
      "source": [
        "def get_data_frames():\n",
        "  df_train_single = pd.read_csv(SINGLE_TRAIN_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "  df_test_single = pd.read_csv(SINGLE_TEST_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "\n",
        "  df_train_multi = pd.read_csv(MULTI_TRAIN_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "  df_test_multi = pd.read_csv(MULTI_TEST_DATAPATH, sep='\\t', quotechar=\"'\", quoting=csv.QUOTE_NONE)\n",
        "\n",
        "  return df_train_single, df_test_single, df_train_multi, df_test_multi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLkKx7GsyGcK"
      },
      "source": [
        "df_train_single, df_test_single, df_train_multi, df_test_multi = get_data_frames()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYq6zO2NPEBt"
      },
      "source": [
        "Features used \n",
        "\n",
        "* Word Embedding [GloVe 50 dimensional embeddings](http://nlp.stanford.edu/data/glove.6B.zip)\n",
        "* Length of word\n",
        "* Syllable count [PyPy](https://pypi.org/project/syllables/)\n",
        "* Word Frequency [PyPy](https://pypi.org/project/wordfreq/)\n",
        "* POS tag [Spacy](https://spacy.io/usage/linguistic-features#pos-tagging)\n",
        "\n",
        "[Reference](https://www.aclweb.org/anthology/W18-0508.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DOS3snOBoin"
      },
      "source": [
        "single_tokens_train_raw = df_train_single[\"token\"].astype(str).to_list()\n",
        "single_tokens_test_raw = df_test_single[\"token\"].astype(str).to_list()\n",
        "\n",
        "y_single_train = df_train_single[\"complexity\"].astype(np.float32).to_numpy()\n",
        "y_single_test = df_test_single[\"complexity\"].astype(np.float32).to_numpy()\n",
        "\n",
        "multi_tokens_train_raw = df_train_multi[\"token\"].astype(str).to_list()\n",
        "multi_tokens_test_raw = df_test_multi[\"token\"].astype(str).to_list()\n",
        "\n",
        "y_multi_train = df_train_multi[\"complexity\"].astype(np.float32).to_numpy()\n",
        "y_multi_test = df_test_multi[\"complexity\"].astype(np.float32).to_numpy()\n",
        "\n",
        "sent_train_single_raw = df_train_single[\"sentence\"].to_list()\n",
        "sent_test_single_raw = df_test_single[\"sentence\"].to_list()\n",
        "\n",
        "sent_train_multi_raw = df_train_multi[\"sentence\"].to_list()\n",
        "sent_test_multi_raw = df_test_multi[\"sentence\"].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z00nnMqBVTmi"
      },
      "source": [
        "EMBEDDING_DIM = 50\n",
        "\n",
        "def get_embeddings():\n",
        "  embedding_index = {}\n",
        "  with open('glove.6B.{}d.txt'.format(EMBEDDING_DIM), 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      token = values[0]\n",
        "      embedding_index[token] = np.asarray(values[1:], dtype='float32')\n",
        "  return embedding_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcpQP9sYzfun",
        "outputId": "68bdf1d6-a92a-4368-bad6-8bd4d2f5cc4f"
      },
      "source": [
        "embedding_index = get_embeddings()\n",
        "print('Token count in embeddings: {}'.format(len(embedding_index)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token count in embeddings: 400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXTRtrC3ptjk"
      },
      "source": [
        "biLSTM to predict target probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eykntycSwUQO"
      },
      "source": [
        "HIDDEN_DIM = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOcp3gYaRFyv"
      },
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "  seq = seq.split()\n",
        "  idxs = [to_ix[w.lower()] if w.lower() in to_ix else len(to_ix) for w in seq]\n",
        "  idxs = torch.tensor(idxs)\n",
        "  idxs = nn.functional.one_hot(idxs, num_classes=len(to_ix))\n",
        "  idxs = torch.tensor(idxs, dtype=torch.float32)\n",
        "  return idxs\n",
        "\n",
        "\n",
        "def map_token_to_idx():\n",
        "  word_to_ix = {}\n",
        "  word_to_ix_multi = {}\n",
        "  for sent in sent_train_single_raw:\n",
        "    sent = sent.split()\n",
        "    for word in sent:\n",
        "      word = word.lower()\n",
        "      if word not in word_to_ix:\n",
        "        word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "  for sent in sent_train_multi_raw:\n",
        "    sent = sent.split()\n",
        "    for word in sent:\n",
        "      word = word.lower()\n",
        "      if word not in word_to_ix_multi:\n",
        "        word_to_ix_multi[word] = len(word_to_ix_multi)\n",
        "  \n",
        "  return word_to_ix, word_to_ix_multi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Td2vdR0JqZ",
        "outputId": "1740f8b9-5158-4c50-82c6-652cef737f70"
      },
      "source": [
        "word_to_ix, word_to_ix_multi = map_token_to_idx()\n",
        "print('SWE vocab size: {}\\nMWE vocab size: {}'.format(len(word_to_ix), len(word_to_ix_multi)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SWE vocab size: 24350\n",
            "MWE vocab size: 9699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOFJH3_us5bj"
      },
      "source": [
        "biLSTM to calculate token probability given context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8rD4jq599bT"
      },
      "source": [
        "class biLSTM(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size):\n",
        "    super(biLSTM, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "    self.hidden2tag = nn.Linear(2 * hidden_dim, output_size)\n",
        "\n",
        "  def prepare_embedding(self, sentence):\n",
        "    embeddings = []\n",
        "    for word in sentence:\n",
        "      word = word.lower()\n",
        "      if word in embedding_index:\n",
        "        embeddings.extend(embedding_index[word])\n",
        "      else:\n",
        "        embeddings.extend(np.random.random(EMBEDDING_DIM).tolist())\n",
        "    embeddings = torch.tensor(embeddings, dtype=torch.float32, device=device)\n",
        "    return embeddings\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    sentence = sentence.split()\n",
        "    embeds = self.prepare_embedding(sentence)\n",
        "    lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "    tag_scores = F.softmax(tag_space, dim=1)\n",
        "    return tag_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrolGYkytD4f"
      },
      "source": [
        "biLSTM model for single word targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8of3r04B5Tn"
      },
      "source": [
        "model = biLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(word_to_ix))\n",
        "path_biLSTM_single = '/content/gdrive/My Drive/TrainedModels/biLSTM.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCAl7MWG4gqN"
      },
      "source": [
        "USE_PRETRAINED_SINGLE_WORD_TARGET_MODEL = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DfKvyAUT8VN",
        "outputId": "0345a024-95c2-48c5-f184-cf406be610d0"
      },
      "source": [
        "if USE_PRETRAINED_SINGLE_WORD_TARGET_MODEL:\n",
        "  print('Using pre-trained biLSTM on single target expressions')\n",
        "  model = torch.load(path_biLSTM_single)\n",
        "  model.eval()\n",
        "else:\n",
        "  print('Training biLSTM on single target expressions')\n",
        "  # Train the model for 10 epochs\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "  for epoch in range(10):\n",
        "    loss_sum = 0\n",
        "    for sentence in sent_train_single_raw:\n",
        "      model.zero_grad()\n",
        "      targets = prepare_sequence(sentence, word_to_ix)\n",
        "      tag_scores = model(sentence)\n",
        "      loss = loss_function(tag_scores, targets)\n",
        "      loss_sum += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print('Epoch: {} Loss: {}'.format(epoch, loss_sum.item()))\n",
        "  # path_biLSTM_single = '/content/gdrive/My Drive/TrainedModels/biLSTM.pt'\n",
        "  # torch.save(model, path_biLSTM_single)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pre-trained biLSTM on single target expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYMs37iTtLDn"
      },
      "source": [
        "biLSTM model for multi word targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1Ypxmd65_GL"
      },
      "source": [
        "model_multi = biLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix_multi), len(word_to_ix_multi))\n",
        "path_biLSTM_multi = '/content/gdrive/My Drive/TrainedModels/biLSTM_multi.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSw5V4vi6Hx0"
      },
      "source": [
        "USE_PRETRAINED_MULTI_WORD_TARGET_MODEL = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2QK4FDT6eL0",
        "outputId": "e0f4cd2c-43ef-4c6e-d3a6-ad2da2e2c345"
      },
      "source": [
        "if USE_PRETRAINED_MULTI_WORD_TARGET_MODEL:\n",
        "  print('Using pre-trained biLSTM on multi target expressions')\n",
        "  model_multi = torch.load(path_biLSTM_multi)\n",
        "  model_multi.eval()\n",
        "else:\n",
        "  print('Training biLSTM on multi target expressions')\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model_multi.parameters(), lr=0.01)\n",
        "  for epoch in range(10):\n",
        "    loss_sum = 0\n",
        "    for sentence in sent_train_multi_raw:\n",
        "      model_multi.zero_grad()\n",
        "      targets = prepare_sequence(sentence, word_to_ix_multi)\n",
        "      tag_scores = model_multi(sentence)\n",
        "      loss = loss_function(tag_scores, targets)\n",
        "      loss_sum += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print('Epoch: {} Loss: {}'.format(epoch, loss_sum.item()))\n",
        "  # path_biLSTM_multi = '/content/gdrive/My Drive/TrainedModels/biLSTM_multi.pt'\n",
        "  # torch.save(model_multi, path_biLSTM_multi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training biLSTM on multi target expressions\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: 0.13678808510303497\n",
            "Epoch: 1 Loss: 0.11500663310289383\n",
            "Epoch: 2 Loss: 0.1051858589053154\n",
            "Epoch: 3 Loss: 0.1026022657752037\n",
            "Epoch: 4 Loss: 0.10051382333040237\n",
            "Epoch: 5 Loss: 0.09855499863624573\n",
            "Epoch: 6 Loss: 0.09558074176311493\n",
            "Epoch: 7 Loss: 0.09392520785331726\n",
            "Epoch: 8 Loss: 0.09278615564107895\n",
            "Epoch: 9 Loss: 0.09134265035390854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5PUts_pSloU"
      },
      "source": [
        "def prepare_features_single_word(tokens, sentences):\n",
        "  features = []\n",
        "  for idx, word in enumerate(tokens):\n",
        "    word = word.lower()\n",
        "    feature = []\n",
        "\n",
        "    # Word length\n",
        "    feature.append(len(word))\n",
        "    doc = nlp(word)\n",
        "\n",
        "    # Syllable count and word frequency in the corpus\n",
        "    # Spacy tokenizes the input sentence\n",
        "    # In this case we would have only one token, the target word\n",
        "    for token in doc:\n",
        "      feature.append(token._.syllables_count)\n",
        "      feature.append(word_frequency(word, 'en'))\n",
        "\n",
        "    # Probability of target word `word` in the sentence estimated from by `model`\n",
        "    if word in word_to_ix:\n",
        "      # Output scores for each of the word in the sentence\n",
        "      out = model(sentences[idx])\n",
        "      pos = -1\n",
        "      for itr, token in enumerate(sentences[idx].split()):\n",
        "        if token.lower() == word:\n",
        "          pos = itr\n",
        "          break\n",
        "      id_pos = word_to_ix[word] # word to id mapping\n",
        "      feature.append(float(out[pos][id_pos]))\n",
        "    else:\n",
        "      # `word` not in vocabulary, so cannot predict probability in context\n",
        "      feature.append(0.0)\n",
        "\n",
        "    # GloVE embedding for the `word`\n",
        "    if word in embedding_index:\n",
        "      feature.extend(embedding_index[word].tolist())\n",
        "    else:\n",
        "      # `word` not in the GloVE corpus, take a random embedding\n",
        "      feature.extend(np.random.random(EMBEDDING_DIM).tolist())\n",
        "    features.append(feature)\n",
        "\n",
        "    if (idx + 1) % 500 == 0:\n",
        "      print('Prepared features for {} single target word sentences'.format(idx + 1))\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5BY3iu53udY"
      },
      "source": [
        "def prepare_features_multi_word(tokens, sentences):\n",
        "  features = []\n",
        "  for idx, word in enumerate(tokens):\n",
        "    word = word.lower()\n",
        "    feature = []\n",
        "    doc = nlp(word)\n",
        "    word = word.split(' ')\n",
        "    assert(len(word) == 2)\n",
        "\n",
        "    # MWE length = sum(length of individual words)\n",
        "    feature.append(len(word[0]) + len(word[1]))\n",
        "\n",
        "    syllables = 0\n",
        "    probability = 1\n",
        "    embedding = np.zeros(EMBEDDING_DIM)\n",
        "\n",
        "    # Syllable count and word frequency in the corpus\n",
        "    # Spacy tokenizes the input sentence\n",
        "    # In this case we would have two tokens\n",
        "\n",
        "    for token in doc:\n",
        "      word_ = token.text\n",
        "      syllables += token._.syllables_count\n",
        "      probability *= word_frequency(word_, 'en')\n",
        "\n",
        "      # GloVE embedding current `word_` of the MWE\n",
        "      if word_ in embedding_index:\n",
        "        embedding = embedding + embedding_index[word_]\n",
        "      else:\n",
        "        # `word_` not in the GloVE corpus, take a random embedding\n",
        "        embedding = embedding + np.random.random(EMBEDDING_DIM)\n",
        "\n",
        "    # Average embedding of the two tokens in the MWE\n",
        "    embedding = embedding / 2\n",
        "    feature.append(syllables)\n",
        "    feature.append(probability)\n",
        "\n",
        "    # Product of probabilities of constituent words in the MWE\n",
        "    if word[0] in word_to_ix_multi and word[1] in word_to_ix_multi:\n",
        "      # Output scores for each of the word in the sentence\n",
        "      out = model_multi(sentences[idx])\n",
        "      pos0, pos1 = -1, -1\n",
        "      for itr, token in enumerate(sentences[idx].split()):\n",
        "        if token.lower() == word[0]:\n",
        "          pos0 = itr\n",
        "          pos1 = itr + 1\n",
        "          break\n",
        "      id_pos0 = word_to_ix_multi[word[0]]\n",
        "      id_pos1 = word_to_ix_multi[word[1]]\n",
        "      feature.append(float(out[pos0][id_pos0] * out[pos1][id_pos1]))\n",
        "    else:\n",
        "      # Either of the constituent words of the MWE not in vocabulary \\\n",
        "      # So cannot predict probability in context\n",
        "      feature.append(0.0)\n",
        "\n",
        "    feature.extend(embedding.tolist())\n",
        "    features.append(feature)\n",
        "\n",
        "    if (idx + 1) % 500 == 0:\n",
        "      print('Prepared features for {} multi target word sentences'.format(idx + 1))\n",
        "\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYuU10C8hZjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f52203-2ad4-4d89-ec0d-bb8560cb6a03"
      },
      "source": [
        "print('+++ Generating Train features for Single word expressions +++')\n",
        "features_train_single = prepare_features_single_word(single_tokens_train_raw, sent_train_single_raw)\n",
        "print('+++ Generating Test features for Single word expressions +++')\n",
        "features_test_single = prepare_features_single_word(single_tokens_test_raw, sent_test_single_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prepared features for 1 single target word sentences\n",
            "Prepared features for 101 single target word sentences\n",
            "Prepared features for 201 single target word sentences\n",
            "Prepared features for 301 single target word sentences\n",
            "Prepared features for 401 single target word sentences\n",
            "Prepared features for 501 single target word sentences\n",
            "Prepared features for 601 single target word sentences\n",
            "Prepared features for 701 single target word sentences\n",
            "Prepared features for 801 single target word sentences\n",
            "Prepared features for 901 single target word sentences\n",
            "Prepared features for 1001 single target word sentences\n",
            "Prepared features for 1101 single target word sentences\n",
            "Prepared features for 1201 single target word sentences\n",
            "Prepared features for 1301 single target word sentences\n",
            "Prepared features for 1401 single target word sentences\n",
            "Prepared features for 1501 single target word sentences\n",
            "Prepared features for 1601 single target word sentences\n",
            "Prepared features for 1701 single target word sentences\n",
            "Prepared features for 1801 single target word sentences\n",
            "Prepared features for 1901 single target word sentences\n",
            "Prepared features for 2001 single target word sentences\n",
            "Prepared features for 2101 single target word sentences\n",
            "Prepared features for 2201 single target word sentences\n",
            "Prepared features for 2301 single target word sentences\n",
            "Prepared features for 2401 single target word sentences\n",
            "Prepared features for 2501 single target word sentences\n",
            "Prepared features for 2601 single target word sentences\n",
            "Prepared features for 2701 single target word sentences\n",
            "Prepared features for 2801 single target word sentences\n",
            "Prepared features for 2901 single target word sentences\n",
            "Prepared features for 3001 single target word sentences\n",
            "Prepared features for 3101 single target word sentences\n",
            "Prepared features for 3201 single target word sentences\n",
            "Prepared features for 3301 single target word sentences\n",
            "Prepared features for 3401 single target word sentences\n",
            "Prepared features for 3501 single target word sentences\n",
            "Prepared features for 3601 single target word sentences\n",
            "Prepared features for 3701 single target word sentences\n",
            "Prepared features for 3801 single target word sentences\n",
            "Prepared features for 3901 single target word sentences\n",
            "Prepared features for 4001 single target word sentences\n",
            "Prepared features for 4101 single target word sentences\n",
            "Prepared features for 4201 single target word sentences\n",
            "Prepared features for 4301 single target word sentences\n",
            "Prepared features for 4401 single target word sentences\n",
            "Prepared features for 4501 single target word sentences\n",
            "Prepared features for 4601 single target word sentences\n",
            "Prepared features for 4701 single target word sentences\n",
            "Prepared features for 4801 single target word sentences\n",
            "Prepared features for 4901 single target word sentences\n",
            "Prepared features for 5001 single target word sentences\n",
            "Prepared features for 5101 single target word sentences\n",
            "Prepared features for 5201 single target word sentences\n",
            "Prepared features for 5301 single target word sentences\n",
            "Prepared features for 5401 single target word sentences\n",
            "Prepared features for 5501 single target word sentences\n",
            "Prepared features for 5601 single target word sentences\n",
            "Prepared features for 5701 single target word sentences\n",
            "Prepared features for 5801 single target word sentences\n",
            "Prepared features for 5901 single target word sentences\n",
            "Prepared features for 6001 single target word sentences\n",
            "Prepared features for 6101 single target word sentences\n",
            "Prepared features for 6201 single target word sentences\n",
            "Prepared features for 6301 single target word sentences\n",
            "Prepared features for 6401 single target word sentences\n",
            "Prepared features for 6501 single target word sentences\n",
            "Prepared features for 6601 single target word sentences\n",
            "Prepared features for 6701 single target word sentences\n",
            "Prepared features for 6801 single target word sentences\n",
            "Prepared features for 6901 single target word sentences\n",
            "Prepared features for 7001 single target word sentences\n",
            "Prepared features for 7101 single target word sentences\n",
            "Prepared features for 7201 single target word sentences\n",
            "Prepared features for 7301 single target word sentences\n",
            "Prepared features for 7401 single target word sentences\n",
            "Prepared features for 7501 single target word sentences\n",
            "Prepared features for 7601 single target word sentences\n",
            "Prepared features for 1 single target word sentences\n",
            "Prepared features for 101 single target word sentences\n",
            "Prepared features for 201 single target word sentences\n",
            "Prepared features for 301 single target word sentences\n",
            "Prepared features for 401 single target word sentences\n",
            "Prepared features for 501 single target word sentences\n",
            "Prepared features for 601 single target word sentences\n",
            "Prepared features for 701 single target word sentences\n",
            "Prepared features for 801 single target word sentences\n",
            "Prepared features for 901 single target word sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqj-99dkA43o",
        "outputId": "e2ba4c00-5ddf-4605-b0aa-e7593a482a66"
      },
      "source": [
        "print('+++ Generating Train features for Multi word expressions +++')\n",
        "features_train_multi = prepare_features_multi_word(multi_tokens_train_raw, sent_train_multi_raw)\n",
        "print('+++ Generating Test features for Multi word expressions +++')\n",
        "features_test_multi = prepare_features_multi_word(multi_tokens_test_raw, sent_test_multi_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prepared features for 500 multi target word sentences\n",
            "Prepared features for 1000 multi target word sentences\n",
            "Prepared features for 1500 multi target word sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESn2VJdU1p8h"
      },
      "source": [
        "# Convert all features to torch.tensor to enable use in PyTorch models\n",
        "X_train_single_tensor = torch.tensor(features_train_single, dtype=torch.float32, device=device)\n",
        "X_test_single_tensor = torch.tensor(features_test_single, dtype=torch.float32, device=device)\n",
        "X_train_multi_tensor = torch.tensor(features_train_multi, dtype=torch.float32, device=device)\n",
        "X_test_multi_tensor = torch.tensor(features_test_multi, dtype=torch.float32, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VikNi2ZmtEn"
      },
      "source": [
        "# Reshape all output complexity scores to single dimension vectors\n",
        "y_single_train = y_single_train.reshape(y_single_train.shape[0], -1)\n",
        "y_single_test = y_single_test.reshape(y_single_test.shape[0], -1)\n",
        "y_multi_train = y_multi_train.reshape(y_multi_train.shape[0], -1)\n",
        "y_multi_test = y_multi_test.reshape(y_multi_test.shape[0], -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhjYgCiA13Z1"
      },
      "source": [
        "# Convert all target outputs to torch.tensor to enable use in PyTorch models\n",
        "Y_train_single_tensor = torch.tensor(y_single_train, dtype=torch.float32, device=device)\n",
        "Y_test_single_tensor = torch.tensor(y_single_test, dtype=torch.float32, device=device)\n",
        "Y_train_multi_tensor = torch.tensor(y_multi_train, dtype=torch.float32, device=device)\n",
        "Y_test_multi_tensor = torch.tensor(y_multi_test, dtype=torch.float32, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSRoIvSU2McF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5876268f-a9b5-49b2-aa6d-73cc8f0a6faa"
      },
      "source": [
        "# Ensure each sample from test and train for single word expression is taken\n",
        "print(X_train_single_tensor.shape)\n",
        "print(X_test_single_tensor.shape)\n",
        "print(Y_train_single_tensor.shape)\n",
        "print(Y_test_single_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([7662, 54])\n",
            "torch.Size([917, 54])\n",
            "torch.Size([7662, 1])\n",
            "torch.Size([917, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7de9O0tZBvqD",
        "outputId": "c1661ed2-29e2-4c76-d518-754a7ed45839"
      },
      "source": [
        "# Ensure each sample from test and train for multi word expression is taken\n",
        "print(X_train_multi_tensor.shape)\n",
        "print(X_test_multi_tensor.shape)\n",
        "print(Y_train_multi_tensor.shape)\n",
        "print(Y_test_multi_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1517, 54])\n",
            "torch.Size([184, 54])\n",
            "torch.Size([1517, 1])\n",
            "torch.Size([184, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONmt2TBCK9jO"
      },
      "source": [
        "def convert_tensor_to_np(y):\n",
        "  if device == torch.device(\"cuda\"):\n",
        "    y = y.cpu()\n",
        "  y = y.detach().numpy()\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m07BB6hLc70"
      },
      "source": [
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QVP68p13j5C"
      },
      "source": [
        "# Evaluate the metrics upon which the model would be evaluated\n",
        "def evaluate_metrics(labels, predicted):\n",
        "  vx, vy = [], []\n",
        "  if torch.is_tensor(labels):\n",
        "    vx = labels.clone()\n",
        "    vx = convert_tensor_to_np(vx)\n",
        "  else:\n",
        "    vx = deepcopy(labels)\n",
        "  if torch.is_tensor(predicted):\n",
        "    vy = predicted.clone()\n",
        "    vy = convert_tensor_to_np(vy)\n",
        "  else:\n",
        "    vy = deepcopy(predicted)\n",
        "\n",
        "  pearsonR = np.corrcoef(vx.T, vy.T)[0, 1]\n",
        "  spearmanRho = stats.spearmanr(vx, vy)\n",
        "  MSE = np.mean((vx - vy) ** 2)\n",
        "  MAE = np.mean(np.absolute(vx - vy))\n",
        "  RSquared = pearsonR ** 2\n",
        "\n",
        "  print(\"Peason's R: {}\".format(pearsonR))\n",
        "  print(\"Spearman's rho: {}\".format(spearmanRho))\n",
        "  print(\"R Squared: {}\".format(RSquared))\n",
        "  print(\"MSE: {}\".format(MSE))\n",
        "  print(\"MAE: {}\".format(MAE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGBqpNE_tUWK"
      },
      "source": [
        "## Neural Network\n",
        "\n",
        "* $N$ input sentences  \n",
        "\n",
        "* d (=EMBEDDING_DIM) word embedding\n",
        "\n",
        "* $I$ = Word Embedding matrix ($N \\times d$)\n",
        "\n",
        "* $W_1, W_2, W_3, W_4 := (d \\times 256), (256 \\times 128), (128 \\times 64), (64 \\times 1)$\n",
        "\n",
        "* Equations\n",
        "\n",
        "  * $o_1 = tanh(I \\times W_1 + b_1)$\n",
        "\n",
        "  * $o_2 = tanh(o_1 \\times W_2 + b_2)$\n",
        "\n",
        "  * $o_3 = tanh(o_2 \\times W_3 + b_3)$\n",
        "\n",
        "  * $o_4 = \\sigma(o_3 \\times W_4)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLhlNCoX2UrA"
      },
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super(NN, self).__init__()\n",
        "    self.linear1 = nn.Linear(embedding_dim, 256, bias=True)\n",
        "    self.linear2 = nn.Linear(256, 128, bias=True)\n",
        "    self.linear3 = nn.Linear(128, 64, bias=True)\n",
        "    self.linear4 = nn.Linear(64, 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = torch.tanh(self.linear1(input))\n",
        "    out = torch.tanh(self.linear2(out))\n",
        "    out = torch.tanh(self.linear3(out))\n",
        "    out = torch.sigmoid(self.linear4(out))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwSTHAPd28r8"
      },
      "source": [
        "loss_function = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAW0QJDkCWvv"
      },
      "source": [
        "model_NN = NN(embedding_dim)\n",
        "model_NN.to(device)\n",
        "path_NN = '/content/gdrive/My Drive/TrainedModels/NN_0.731.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lUaRjKGCkoJ"
      },
      "source": [
        "USE_PRETRAINED_SINGLE_WORD_TARGET_NN = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVCTc8Zf3KoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a7b327-78c8-4f31-b5dd-92a16fd4aa2e"
      },
      "source": [
        "if USE_PRETRAINED_SINGLE_WORD_TARGET_NN:\n",
        "  print('Using pre-trained NN on single target expressions')\n",
        "  model_NN = torch.load(path_NN)\n",
        "  model_NN.eval()\n",
        "else:\n",
        "  print('Training NN on single target expressions...')\n",
        "  embedding_dim = X_train_single_tensor.shape[1]\n",
        "  optimizer = optim.Adam(model_NN.parameters(), lr=0.001)\n",
        "  for epoch in range(30):\n",
        "    optimizer.zero_grad()\n",
        "    out = model_NN(X_train_single_tensor)\n",
        "    loss = loss_function(out, Y_train_single_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Epoch {} : {}\".format(epoch + 1, loss.item()))\n",
        "  # path_NN = '/content/gdrive/My Drive/TrainedModels/NN_0.731.pt'\n",
        "  # torch.save(model_NN, path_NN)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pre-trained NN on single target expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DMsEM5R3cUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "427236b2-34f9-47b7-ee6f-eb21b84dfd5d"
      },
      "source": [
        "out_NN = model_NN(X_test_single_tensor)\n",
        "evaluate_metrics(out_NN, Y_test_single_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peason's R: 0.7302781366028076\n",
            "Spearman's rho: SpearmanrResult(correlation=0.6895781931768435, pvalue=2.284117871635271e-130)\n",
            "R Squared: 0.5333061568000689\n",
            "MSE: 0.007758340798318386\n",
            "MAE: 0.06815263628959656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YOGT0TCELKo"
      },
      "source": [
        "model_NN_multi = NN(embedding_dim)\n",
        "model_NN_multi.to(device)\n",
        "path_NN_multi = '/content/gdrive/My Drive/TrainedModels/NN_multi_0.774.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNZIIOkfDM7E"
      },
      "source": [
        "USE_PRETRAINED_MULTI_WORD_TARGET_NN = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl6L4RlYB-Wj",
        "outputId": "735ca3cb-dc67-4b8e-eeeb-38ce7f58f767"
      },
      "source": [
        "if USE_PRETRAINED_MULTI_WORD_TARGET_NN:\n",
        "  print('Using pre-trained NN on multi target expressions')\n",
        "  model_NN_multi = torch.load(path_NN_multi)\n",
        "  model_NN_multi.eval()\n",
        "else:\n",
        "  print('Training NN on multi target expressions...')\n",
        "  embedding_dim = X_train_multi_tensor.shape[1]\n",
        "  optimizer = optim.Adam(model_NN_multi.parameters(), lr=0.001)\n",
        "  for epoch in range(30):\n",
        "    optimizer.zero_grad()\n",
        "    out = model_NN_multi(X_train_multi_tensor)\n",
        "    loss = loss_function(out, Y_train_multi_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Epoch {} : {}\".format(epoch + 1, loss.item()))\n",
        "  # path_NN_multi = '/content/gdrive/My Drive/TrainedModels/NN_multi_0.774.pt'\n",
        "  # torch.save(model_NN_multi, path_NN_multi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using pre-trained NN on multi target expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msBIen6KCGY2",
        "outputId": "b3ae3df3-995d-490b-e0c6-d8b7df45b8a8"
      },
      "source": [
        "out_NN_multi = model_NN_multi(X_test_multi_tensor)\n",
        "evaluate_metrics(out_NN_multi, Y_test_multi_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Peason's R: 0.7742502859156036\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7718377190311184, pvalue=1.2273362586858998e-37)\n",
            "R Squared: 0.5994635052403939\n",
            "MSE: 0.009973190724849701\n",
            "MAE: 0.07805462926626205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SpGwzvfAQ0y"
      },
      "source": [
        "## Machine Learning Methods\n",
        "\n",
        "* Linear Regression\n",
        "\n",
        "* Support Vector Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8RRwfzp0R-K"
      },
      "source": [
        "X_train_single_np = np.array(features_train_single)\n",
        "X_test_single_np = np.array(features_test_single)\n",
        "Y_train_single_np = np.array(y_single_train.reshape(y_single_train.shape[0], -1))\n",
        "Y_test_single_np = np.array(y_single_test.reshape(y_single_test.shape[0], -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2LzJmZwoFMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9765967-a2e0-493c-e029-adfc709ca51f"
      },
      "source": [
        "print(X_train_single_np.shape)\n",
        "print(X_test_single_np.shape)\n",
        "print(Y_train_single_np.shape)\n",
        "print(Y_test_single_np.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7662, 54)\n",
            "(917, 54)\n",
            "(7662, 1)\n",
            "(917, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AW_YHxS73xW"
      },
      "source": [
        "X_train_multi_np = np.array(features_train_multi)\n",
        "X_test_multi_np = np.array(features_test_multi)\n",
        "Y_train_multi_np = np.array(y_multi_train.reshape(y_multi_train.shape[0], -1))\n",
        "Y_test_multi_np = np.array(y_multi_test.reshape(y_multi_test.shape[0], -1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQP30U5JZ6Qi",
        "outputId": "9aa43abe-590d-4428-e533-d2a13bec5851"
      },
      "source": [
        "print(X_train_multi_np.shape)\n",
        "print(X_test_multi_np.shape)\n",
        "print(Y_train_multi_np.shape)\n",
        "print(Y_test_multi_np.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1517, 54)\n",
            "(184, 54)\n",
            "(1517, 1)\n",
            "(184, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwsHCYnr3D3E"
      },
      "source": [
        "### Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ATmatOhioJI"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPRzjdJtOWlM"
      },
      "source": [
        "def evaluateLinearRegression(X_train, Y_train, X_test, Y_test):\n",
        "  reg = LinearRegression().fit(X_train, Y_train)\n",
        "  out = reg.predict(X_test)\n",
        "  evaluate_metrics(out, Y_test)\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMSTucrSOpk7",
        "outputId": "a38faeaf-4424-4284-828a-cab61de52504"
      },
      "source": [
        "print('Linear Regression for Single word expressions')\n",
        "out_LR = evaluateLinearRegression(X_train_single_np, Y_train_single_np, X_test_single_np, Y_test_single_np)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression for Single word expressions\n",
            "Peason's R: 0.711102426607668\n",
            "Spearman's rho: SpearmanrResult(correlation=0.6916354331070241, pvalue=1.8969768901488292e-131)\n",
            "R Squared: 0.5056666611273138\n",
            "MSE: 0.008003885282985275\n",
            "MAE: 0.06849867183285906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoDpEh5ZPA6B",
        "outputId": "c1dd8a42-d958-4e41-af9b-ab873c2e76b9"
      },
      "source": [
        "print('Linear Regression for Multi word expressions')\n",
        "out_LR_multi = evaluateLinearRegression(X_train_multi_np, Y_train_multi_np, X_test_multi_np, Y_test_multi_np)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression for Multi word expressions\n",
            "Peason's R: 0.7672062890328821\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7568121347294066, pvalue=1.91114531951169e-35)\n",
            "R Squared: 0.5886054899316062\n",
            "MSE: 0.009942286822619249\n",
            "MAE: 0.07999100274422145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot7P6ba-3Gwx"
      },
      "source": [
        "### Support Vector Regressor\n",
        "\n",
        "* Radial basis function\n",
        "* C = 0.05\n",
        "* epsilon = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I21y5EAG2BHY"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyTiLWD33PH1"
      },
      "source": [
        "def evaluateSVR(X_train, Y_train, X_test, Y_test):\n",
        "  svr = make_pipeline(StandardScaler(), SVR(C=0.05, epsilon=0.01))\n",
        "  svr.fit(X_train, Y_train.reshape(-1))\n",
        "  out = svr.predict(X_test)\n",
        "  evaluate_metrics(out, Y_test)\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffGoiNKq3hBd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f00dff1-3d80-4b79-8e1f-eff346c0497c"
      },
      "source": [
        "print('SVR for Single word expressions')\n",
        "out_svr = evaluateSVR(X_train_single_np, Y_train_single_np, X_test_single_np, Y_test_single_np)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVR for Single word expressions\n",
            "Peason's R: 0.7335264971754396\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7037698873017006, pvalue=5.1768293993200146e-138)\n",
            "R Squared: 0.5380611220584702\n",
            "MSE: 0.023811804002468438\n",
            "MAE: 0.11936762600868481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWFRhtSEPl9t",
        "outputId": "fb36b383-2bd0-4559-eb97-827be116173d"
      },
      "source": [
        "print('SVR for Multi word expressions')\n",
        "out_svr_multi = evaluateSVR(X_train_multi_np, Y_train_multi_np, X_test_multi_np, Y_test_multi_np)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVR for Multi word expressions\n",
            "Peason's R: 0.7837763189535243\n",
            "Spearman's rho: SpearmanrResult(correlation=0.7555502272808502, pvalue=2.8717808798403713e-35)\n",
            "R Squared: 0.6143053181523367\n",
            "MSE: 0.033322201777695765\n",
            "MAE: 0.14654661696744523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhJgiYbCCpJf"
      },
      "source": [
        "### Summary \n",
        "\n",
        "\n",
        "| Model | Type | Pearson | MSE | MAE|\n",
        "|---|---|---|---|---|\n",
        "|Neural Network| Single word| 0.7302 | 0.0077 | 0.0681|\n",
        "|Neural Network| Multi word| 0.7742 | 0.0100 | 0.0780|\n",
        "|Linear Regression| Single word| 0.7111 | 0.0080 | 0.0684|\n",
        "|Linear Regression| Multi word| 0.7672 | 0.0099 | 0.0800|\n",
        "|SVR| Single word| 0.7335 | 0.0238 | 0.1194|\n",
        "# |SVR| Multi word| 0.7838 | 0.0333 | 0.1465|\n",
        "\n"
      ]
    }
  ]
}