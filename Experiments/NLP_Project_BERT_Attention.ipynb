{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP Project BERT Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpiJBaetUSlX"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtNeO3LJbI3T"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import csv\n",
        "from scipy import stats\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtZUQ9bgc6hf"
      },
      "source": [
        "TRAIN_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/train/lcp_single_train.tsv\"\n",
        "TEST_DATAPATH = \"https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/test-labels/lcp_single_test.tsv\"\n",
        "train_filepath = \"/content/sample_data/train.csv\"\n",
        "test_filepath = \"/content/sample_data/test.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKMZicAII8iu"
      },
      "source": [
        "k = 1            # token append number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxYGarkmP47Z"
      },
      "source": [
        "df_train = pd.read_csv(TRAIN_DATAPATH, sep = '\\t', quotechar=\"'\", quoting = csv.QUOTE_NONE)\n",
        "df_test = pd.read_csv(TEST_DATAPATH, sep = '\\t', quotechar=\"'\", quoting = csv.QUOTE_NONE)\n",
        "df_train['complexity'] = df_train['complexity'].astype(float)\n",
        "df_test['complexity'] = df_test['complexity'].astype(float)\n",
        "for i in range(len(df_train)):\n",
        "    first = str(df_train['token'][i]) + \" [SEP] \"\n",
        "    last = \" [SEP] \" + str(df_train['token'][i])\n",
        "    for _ in range(k):\n",
        "        df_train['sentence'][i] = first + df_train['sentence'][i] + last\n",
        "for i in range(len(df_test)):\n",
        "    first = str(df_test['token'][i]) + \" [SEP] \"\n",
        "    last = \" [SEP] \" + str(df_test['token'][i])\n",
        "    for _ in range(k):\n",
        "        df_test['sentence'][i] = first + df_test['sentence'][i] + last\n",
        "df_train = df_train.drop(['id', 'corpus', 'token'], axis = 1)\n",
        "df_test = df_test.drop(['id', 'corpus', 'token'], axis = 1)\n",
        "df_train = df_train[['complexity', 'sentence']]\n",
        "df_test = df_test[['complexity', 'sentence']]\n",
        "df_train.to_csv(train_filepath, index = False)\n",
        "df_test.to_csv(test_filepath, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za0-FKknnt4E"
      },
      "source": [
        "device = 'cuda'\n",
        "batch_size = 8\n",
        "num_epochs = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IU4U-WhP9Gc"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvgOSFuvbXx5"
      },
      "source": [
        "label = Field(sequential = False, use_vocab = False, batch_first = True, dtype = torch.float32)\n",
        "text = Field(use_vocab = False, tokenize = tokenizer.encode, lower = False, batch_first = True, pad_token = PAD_INDEX, unk_token = UNK_INDEX)\n",
        "fields = [('complexity', label), ('sentence', text)]\n",
        "train = TabularDataset(path = train_filepath, format = 'csv', skip_header = True, fields = fields)\n",
        "train_iter = BucketIterator(train, batch_size = batch_size, device = device, sort_key = lambda x: len(x.sentence), train = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7thaDcaTXky"
      },
      "source": [
        "test_label = Field(sequential = False, use_vocab = False, batch_first = True, dtype = torch.float32)\n",
        "test_text = Field(use_vocab = False, tokenize = tokenizer.encode, lower = False, batch_first = True, pad_token = PAD_INDEX, unk_token = UNK_INDEX)\n",
        "test_fields = [('complexity', test_label), ('sentence', test_text)]\n",
        "test = TabularDataset(path = test_filepath, format = 'csv', skip_header = True, fields = test_fields)\n",
        "test_iter = BucketIterator(test, batch_size = batch_size, device = device, sort_key = lambda x: len(x.sentence), train = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLKZQSUeodOJ"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "model.config.num_labels = 1\n",
        "model.config.output_attentions = True\n",
        "model.to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWilrrIhvzWa"
      },
      "source": [
        "def train_bert(model, iterator):\n",
        "    epoch_loss = 0.0\n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        text = batch.sentence\n",
        "        label = batch.complexity\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text)                       \n",
        "        logits = output.logits[:, : 1]                    \n",
        "        logits = torch.sigmoid(torch.squeeze(logits))\n",
        "        attentions = torch.max(torch.max(output.attentions[-1], 3).values, 2).values              # takes maximum attention score from self attentions across heads in last layer of BERT\n",
        "        try:\n",
        "            predicted.extend(logits.tolist())\n",
        "            labels.extend(label.tolist())\n",
        "            attention_scores.extend(attentions.tolist())\n",
        "            loss = criterion(label, logits)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        except TypeError:\n",
        "            pass\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t55EKZ0AUMTl"
      },
      "source": [
        "def test_bert(model, iterator):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text = batch.sentence\n",
        "            label = batch.complexity\n",
        "            output = model(text)\n",
        "            logits = output.logits[:, : 1]                    \n",
        "            logits = torch.sigmoid(torch.squeeze(logits))\n",
        "            attentions = torch.max(torch.max(output.attentions[-1], 3).values, 2).values \n",
        "            try:\n",
        "                test_predicted.extend(logits.tolist())\n",
        "                test_labels.extend(label.tolist())\n",
        "                test_attention_scores.extend(attentions.tolist())\n",
        "            except TypeError:\n",
        "                pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6mtsea4tpBj"
      },
      "source": [
        "def calculate_metrics(y, y_hat):\n",
        "    vx = y.astype(float)\n",
        "    vy = y_hat.astype(float)\n",
        "    pearsonR = np.corrcoef(vx, vy)[0, 1]\n",
        "    spearmanRho = stats.spearmanr(vx, vy)\n",
        "    MSE = np.mean((vx - vy) ** 2)\n",
        "    MAE = np.mean(np.absolute(vx - vy))\n",
        "    RSquared = (pearsonR ** 2)\n",
        "\n",
        "    print(\"Pearson's R: {}\".format(pearsonR))\n",
        "    print(\"Spearman's rho: {}\".format(spearmanRho))\n",
        "    print(\"R Squared: {}\".format(RSquared))\n",
        "    print(\"MSE: {}\".format(MSE))\n",
        "    print(\"MAE: {}\".format(MAE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpG1HHmIwDF1"
      },
      "source": [
        "attention_scores = []\n",
        "labels = []\n",
        "test_attention_scores = []\n",
        "test_labels = []\n",
        "test_predicted = []\n",
        "print(\"------BERT Training------\")\n",
        "for epoch in range(num_epochs):\n",
        "    labels = []\n",
        "    predicted = []\n",
        "    attention_scores = []\n",
        "    train_loss = train_bert(model, train_iter)\n",
        "    print(f'\\t Epoch: {epoch + 1} | Train Loss: {train_loss: }')\n",
        "    calculate_metrics(np.array(labels), np.array(predicted))\n",
        "print(\"------BERT Test----------\")\n",
        "test_bert(model, test_iter)\n",
        "calculate_metrics(np.array(test_labels), np.array(test_predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkWUtfiEgIZ3"
      },
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(input_dim, 24)\n",
        "    self.linear2 = nn.Linear(24, 48)\n",
        "    self.linear3 = nn.Linear(48, 96)\n",
        "    self.linear4 = nn.Linear(96, 96)\n",
        "    self.linear5 = nn.Linear(96, 48)\n",
        "    self.linear6 = nn.Linear(48, 24)\n",
        "    self.linear7 = nn.Linear(24, 12)\n",
        "    self.linear8 = nn.Linear(12, 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    out = F.sigmoid(self.linear1(input))\n",
        "    out = F.sigmoid(self.linear2(out))\n",
        "    out = F.sigmoid(self.linear3(out))\n",
        "    out = F.sigmoid(self.linear4(out))\n",
        "    out = F.sigmoid(self.linear5(out))\n",
        "    out = F.sigmoid(self.linear6(out))\n",
        "    out = F.sigmoid(self.linear7(out))\n",
        "    out = F.sigmoid(self.linear8(out))\n",
        "    out = torch.squeeze(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48LU3FgHmFy8"
      },
      "source": [
        "nn_num_epochs = 200\n",
        "nn_input = torch.tensor(attention_scores, device = device, requires_grad = True)\n",
        "labels = torch.tensor(labels, device = device, requires_grad = True)\n",
        "nn_input_test = torch.tensor(test_attention_scores, device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeC6Qnnsgad_"
      },
      "source": [
        "input_dim = len(attention_scores[0])\n",
        "nn_model = NN(input_dim)\n",
        "nn_model.to(device)\n",
        "nn_criterion = nn.MSELoss()\n",
        "nn_optimizer = optim.Adam(nn_model.parameters(), lr = 0.00001)\n",
        "nn_scheduler = torch.optim.lr_scheduler.StepLR(nn_optimizer, step_size = 10, gamma = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W7ptN6HjqgK"
      },
      "source": [
        "def train_nn(nn_model, input):\n",
        "    nn_model.train()\n",
        "    nn_optimizer.zero_grad()\n",
        "    output = nn_model(input)                       \n",
        "    loss = nn_criterion(labels, output)\n",
        "    loss.backward()\n",
        "    nn_optimizer.step()\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQEVXfJPp5y8"
      },
      "source": [
        "def test_nn(nn_model, input):\n",
        "    nn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = nn_model(input)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGxPQtbXl9kA"
      },
      "source": [
        "for epoch in range(nn_num_epochs):\n",
        "    nn_train_loss = train_nn(nn_model, nn_input)\n",
        "    print(\"Epoch {} : {}\".format(epoch + 1, nn_train_loss))\n",
        "    print(\"------Metrics for test------\")\n",
        "    output = test_nn(nn_model, nn_input_test)\n",
        "    calculate_metrics(np.array(test_labels), np.array(output.tolist()))\n",
        "    nn_scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfbDAQS6XLDQ"
      },
      "source": [
        "Only single words have been trained here because doing both single and multi words in same session may lead to CUDA Out Of Memory error, as the attention scores are quite memory intensive to store\n"
      ]
    }
  ]
}