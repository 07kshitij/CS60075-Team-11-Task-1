# -*- coding: utf-8 -*-
"""BERT Sentence Transformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ERN7Tr7qNpyNmyBZ6dLRtK7Pwvkgkrm4
"""

!pip install transformers
!pip install sentence-transformers

import torch_xla
import torch_xla.core.xla_model as xm

import numpy as np
import pandas as pd
import torch
import csv
from scipy import stats
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from sentence_transformers import SentenceTransformer

TRAIN_DATAPATH = "https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/train/lcp_single_train.tsv"
TEST_DATAPATH = "https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/test-labels/lcp_single_test.tsv"
df_train = pd.read_csv(TRAIN_DATAPATH, sep = '\t', quotechar="'", quoting = csv.QUOTE_NONE)
df_test = pd.read_csv(TEST_DATAPATH, sep = '\t', quotechar="'", quoting = csv.QUOTE_NONE)
df_train['complexity'] = df_train['complexity'].astype(float)
df_test['complexity'] = df_test['complexity'].astype(float)

k = 1                        # token append number
device = 'cuda'

for i in range(len(df_train)):
    last = " " + str(df_train['token'][i])
    for _ in range(k):
        df_train['sentence'][i] = df_train['sentence'][i] + last
for i in range(len(df_test)):
    last = " " + str(df_test['token'][i])
    for _ in range(k):
        df_test['sentence'][i] = df_test['sentence'][i] + last

df_train

model = SentenceTransformer('paraphrase-distilroberta-base-v1')

train_input = [i for i in df_train['sentence']]
test_input = [i for i in df_test['sentence']]
labels = [i for i in df_train['complexity']]
test_labels = [i for i in df_test['complexity']]
train_emb = model.encode(train_input)
test_emb = model.encode(test_input)

class NN(nn.Module):
  def __init__(self, input_dim):
    super().__init__()
    self.linear1 = nn.Linear(input_dim, 1536)
    self.linear2 = nn.Linear(1536, 3072)
    self.linear3 = nn.Linear(3072, 3072)
    self.linear4 = nn.Linear(3072, 1536)
    self.linear5 = nn.Linear(1536, 768)
    self.linear6 = nn.Linear(768, 768)
    self.linear7 = nn.Linear(768, 768)
    self.linear8 = nn.Linear(768, 256)
    self.linear9 = nn.Linear(256, 128)
    self.linear10 = nn.Linear(128, 64)
    self.linear11 = nn.Linear(64, 1)

  def forward(self, input):                                      # gelu in initial layers is quite good, gives 0.5 r
    out = F.elu(self.linear1(input))
    out = F.elu(self.linear2(out))
    out = F.elu(self.linear3(out))
    out = F.elu(self.linear4(out))
    out = F.elu(self.linear5(out))
    out = F.elu(self.linear6(out))
    out = F.elu(self.linear7(out))
    out = F.elu(self.linear8(out))
    out = F.elu(self.linear9(out))
    out = F.elu(self.linear10(out))
    out = F.sigmoid(self.linear11(out))
    out = torch.squeeze(out)
    return out

def pearson_loss(target, output):
    eps = 0.000001
    output_mean = torch.mean(output)
    target_mean = torch.mean(target)
    x = output - output_mean.expand_as(output)
    y = target - target_mean.expand_as(target)
    pearson = torch.dot(x, y)/ ((torch.std(output) + eps) * (torch.std(target) + eps))
    loss = (-1.0 * pearson / len(target))
    return loss

nn_num_epochs = 1500
nn_input = torch.tensor(train_emb, device = device, requires_grad = True)
labels = torch.tensor(labels, dtype = torch.float32, device = device, requires_grad = True)
nn_input_test = torch.tensor(test_emb, device = device)

input_dim = len(train_emb[0])
print("Input Dimension of NN: " + str(input_dim))
nn_model = NN(input_dim)
nn_model.to(device)
#nn_criterion = nn.MSELoss()
nn_optimizer = optim.Adam(nn_model.parameters(), lr = 0.00001)
#nn_scheduler = torch.optim.lr_scheduler.StepLR(nn_optimizer, step_size = 100, gamma = 2)

def train_nn(nn_model, input):
    nn_model.train()
    nn_optimizer.zero_grad()
    output = nn_model(input)                       
    loss = pearson_loss(labels, output)
    loss.backward()
    nn_optimizer.step()
    return loss.item()

def test_nn(nn_model, input):
    nn_model.eval()
    with torch.no_grad():
        output = nn_model(input)
        return output

def calculate_metrics(y, y_hat):
    vx = y.astype(float)
    vy = y_hat.astype(float)
    pearsonR = np.corrcoef(vx, vy)[0, 1]
    spearmanRho = stats.spearmanr(vx, vy)
    MSE = np.mean((vx - vy) ** 2)
    MAE = np.mean(np.absolute(vx - vy))
    RSquared = (pearsonR ** 2)

    print("Peason's R: {}".format(pearsonR))
    print("Spearman's rho: {}".format(spearmanRho))
    print("R Squared: {}".format(RSquared))
    print("MSE: {}".format(MSE))
    print("MAE: {}".format(MAE))

for epoch in range(nn_num_epochs):
    nn_train_loss = train_nn(nn_model, nn_input)
    print("Epoch {} : {}".format(epoch + 1, nn_train_loss))
    output = test_nn(nn_model, nn_input_test)
    calculate_metrics(np.array(test_labels), np.array(output.tolist()))
    nn_scheduler.step()