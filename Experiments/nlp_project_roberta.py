# -*- coding: utf-8 -*-
"""NLP Project RoBERTa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xCUnHC-jk5B1CgCTCqhEfZUJqq6N8RKn
"""

!pip install transformers
import numpy as np
import pandas as pd
import torch
import csv
from scipy import stats
from transformers import RobertaTokenizer, RobertaForSequenceClassification
import torch.nn as nn
import torch.optim as optim
from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator

TRAIN_DATAPATH = "https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/train/lcp_single_train.tsv"
TEST_DATAPATH = "https://raw.githubusercontent.com/MMU-TDMLab/CompLex/master/test-labels/lcp_single_test.tsv"
train_filepath = "/content/sample_data/train.csv"
test_filepath = "/content/sample_data/test.csv"

k = 1            # token append number

df_train = pd.read_csv(TRAIN_DATAPATH, sep = '\t', quotechar="'", quoting = csv.QUOTE_NONE)
df_test = pd.read_csv(TEST_DATAPATH, sep = '\t', quotechar="'", quoting = csv.QUOTE_NONE)
df_train['complexity'] = df_train['complexity'].astype(float)
df_test['complexity'] = df_test['complexity'].astype(float)
for i in range(len(df_train)):
    first = str(df_train['token'][i]) + " [SEP] "
    last = " [SEP] " + str(df_train['token'][i])
    for _ in range(k):
        df_train['sentence'][i] = first + df_train['sentence'][i] + last
for i in range(len(df_test)):
    first = str(df_test['token'][i]) + " [SEP] "
    last = " [SEP] " + str(df_test['token'][i])
    for _ in range(k):
        df_test['sentence'][i] = first + df_test['sentence'][i] + last
df_train = df_train.drop(['id', 'corpus', 'token'], axis = 1)
df_test = df_test.drop(['id', 'corpus', 'token'], axis = 1)
df_train = df_train[['complexity', 'sentence']]
df_test = df_test[['complexity', 'sentence']]
df_train.to_csv(train_filepath, index = False)
df_test.to_csv(test_filepath, index = False)

device = 'cuda'
batch_size = 4
num_epochs = 10

tokenizer = RobertaTokenizer.from_pretrained("roberta-large")

PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)

label = Field(sequential = False, use_vocab = False, batch_first = True, dtype = torch.float32)
text = Field(use_vocab = False, tokenize = tokenizer.encode, lower = False, batch_first = True, pad_token = PAD_INDEX, unk_token = UNK_INDEX)
fields = [('complexity', label), ('sentence', text)]
train = TabularDataset(path = train_filepath, format = 'csv', skip_header = True, fields = fields)
train_iter = BucketIterator(train, batch_size = batch_size, sort_key = lambda x: len(x.sentence), device = device, train = True, sort = True, sort_within_batch = True)

test_label = Field(sequential = False, use_vocab = False, batch_first = True, dtype = torch.float32)
test_text = Field(use_vocab = False, tokenize = tokenizer.encode, lower = False, batch_first = True, pad_token = PAD_INDEX, unk_token = UNK_INDEX)
test_fields = [('complexity', test_label), ('sentence', test_text)]
test = TabularDataset(path = test_filepath, format = 'csv', skip_header = True, fields = test_fields)
test_iter = BucketIterator(test, batch_size = batch_size, sort_key = lambda x: len(x.sentence), device = device, train = False, sort = True, sort_within_batch = True)

model = RobertaForSequenceClassification.from_pretrained("roberta-large")
model.config.num_labels = 1
model.to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr = 0.00001)

def train_model(model, iterator):
    epoch_loss = 0.0
    epoch_acc = 0.0
    model.train()
    for batch in iterator:
        text = batch.sentence
        label = batch.complexity
        optimizer.zero_grad()
        output = model(text)                       
        logits = output.logits[:, : 1]                    # output gives 2 values of logits, not quite sure what are they
        logits = torch.sigmoid(torch.squeeze(logits))
        try:
            predicted.extend(logits.tolist())
            labels.extend(label.tolist())
            loss = criterion(label, logits)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        except TypeError:
            pass
    return epoch_loss / len(iterator)

def test_model(model, iterator):
    model.eval()
    with torch.no_grad():
        for batch in iterator:
            text = batch.sentence
            label = batch.complexity
            output = model(text)
            logits = output.logits[:, : 1]                    # gives 2 values of logits, not quite sure what are they
            logits = torch.sigmoid(torch.squeeze(logits))
            try:
                test_predicted.extend(logits.tolist())
                test_labels.extend(label.tolist())
            except TypeError:
                pass

def calculate_metrics(y, y_hat):
    vx = y.astype(float)
    vy = y_hat.astype(float)
    pearsonR = np.corrcoef(vx, vy)[0, 1]
    spearmanRho = stats.spearmanr(vx, vy)
    MSE = np.mean((vx - vy) ** 2)
    MAE = np.mean(np.absolute(vx - vy))
    RSquared = (pearsonR ** 2)

    print("Peason's R: {}".format(pearsonR))
    print("Spearman's rho: {}".format(spearmanRho))
    print("R Squared: {}".format(RSquared))
    print("MSE: {}".format(MSE))
    print("MAE: {}".format(MAE))

for epoch in range(num_epochs):
    labels = []
    predicted = []
    train_loss = train_model(model, train_iter)
    print(f'\t Epoch: {epoch + 1} | Train Loss: {train_loss: }')
    print("------Metrics for train------")
    calculate_metrics(np.array(labels), np.array(predicted))
    test_labels = []
    test_predicted = []
    test_model(model, test_iter)
    print("------Metrics for test-------")
    calculate_metrics(np.array(test_labels), np.array(test_predicted))

#shape of attentions = num_epochs * (iterator_size / batch_size) * batch_size * MAX_SEQ_LEN * MAX_SEQ_LEN

#batch = next(iter(train_iter))

#batch.complexity